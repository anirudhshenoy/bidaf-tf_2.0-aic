{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit : https://github.com/ParikhKadam/bidaf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to : https://www.kaggle.com/sanjay11100/squad-stanford-q-a-json-to-pandas-dataframe\n",
    "# Modified to include first answer_start and text for dev set\n",
    "\n",
    "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    js['q_idx'] = ndx\n",
    "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main\n",
    "\n",
    "\n",
    "def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "#     js['q_idx'] = ndx\n",
    "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    answer_start = []\n",
    "    answer_text = []\n",
    "\n",
    "    for answers in tqdm_notebook(main['answers'].values):\n",
    "        answer_start.append(answers[0]['answer_start'])\n",
    "        answer_text.append(answers[0]['text'])\n",
    "\n",
    "    main['answer_start'] = answer_start\n",
    "    main['text'] = answer_text\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "processing...\n",
      "shape of the dataframe is (87599, 6)\n",
      "Done\n",
      "Reading the json file\n",
      "processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73503b35c2a747f6a5a7a33444208d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10570), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape of the dataframe is (10570, 7)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "df_train = squad_json_to_dataframe_train('./data/train-v1.1.json')\n",
    "df_dev = squad_json_to_dataframe_dev('./data/dev-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NUM_SAMPLES = 10 #df_train.shape[0]\n",
    "DEV_NUM_SAMPLES = 10 #df_dev.shape[0]\n",
    "\n",
    "\n",
    "df_train = df_train[:TRAIN_NUM_SAMPLES]\n",
    "df_dev = df_dev[:DEV_NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65514bb1e9774636a4066701ca8d7502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77d568189c44045a58285fb645da6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "\n",
    "answer_start = []\n",
    "answer_end = []\n",
    "for i in tqdm_notebook(range(df_train.shape[0])): \n",
    "    context_split = word_tokenize(df_train.context.values[i][:df_train.answer_start.values[i]])\n",
    "    answer_start.append(len(context_split))\n",
    "    answer_end.append(len(context_split) + len(word_tokenize(df_train.text.values[i])))\n",
    "df_train['answer_end'] = answer_end\n",
    "df_train['answer_start'] = answer_start\n",
    "\n",
    "answer_start = []\n",
    "answer_end = []\n",
    "for i in tqdm_notebook(range(df_dev.shape[0])): \n",
    "    context_split = word_tokenize(df_dev.context.values[i][:df_dev.answer_start.values[i]])\n",
    "    answer_start.append(len(context_split))\n",
    "    answer_end.append(len(context_split) + len(word_tokenize(df_dev.text.values[i])))\n",
    "df_dev['answer_end'] = answer_end\n",
    "df_dev['answer_start'] = answer_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q_train = max([len(word_tokenize(t)) for t in df_train.question.values])\n",
    "max_c_train = max([len(word_tokenize(t)) for t in df_train.context.values])\n",
    "\n",
    "max_q_dev = max([len(word_tokenize(t)) for t in df_dev.question.values])\n",
    "max_c_dev = max([len(word_tokenize(t)) for t in df_dev.context.values])\n",
    "\n",
    "\n",
    "MAX_QUESTION_LENGTH = max(max_q_train, max_q_dev)\n",
    "MAX_CONTEXT_LENGTH = max(max_c_train, max_c_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will download magnitude files from the server if they aren't avaialble locally.. So, grab a cup of coffee while the downloading is under progress..\n"
     ]
    }
   ],
   "source": [
    "from pymagnitude import MagnitudeUtils\n",
    "from scripts import MagnitudeVectors\n",
    "\n",
    "vectors = MagnitudeVectors(50).load_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.answer_start.values, df_train.answer_end.values\n",
    "x_train = df_train.context.values, df_train.question.values\n",
    "\n",
    "y_dev = df_dev.answer_start.values, df_dev.answer_end.values\n",
    "x_dev = df_dev.context.values, df_dev.question.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "num_batches_per_epoch_train = int(np.ceil(TRAIN_NUM_SAMPLES/float(BATCH_SIZE)))\n",
    "num_batches_per_epoch_dev = int(np.ceil(DEV_NUM_SAMPLES/float(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class BatchGenerator(Sequence):\n",
    "    \"Generator for fit_generator\"\n",
    "    \n",
    "    def __init__(self, X, Y, batch_size, MAX_CONTEXT_LENGTH , MAX_QUESTION_LENGTH, NUM_SAMPLES):\n",
    "        self.batch_size = batch_size\n",
    "        self.MAX_CONTEXT_LENGTH = MAX_CONTEXT_LENGTH\n",
    "        self.MAX_QUESTION_LENGTH = MAX_QUESTION_LENGTH\n",
    "        self.NUM_SAMPLES = NUM_SAMPLES\n",
    "        self.num_batches = self.NUM_SAMPLES // self.batch_size\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        #self.shuffle = shuffle\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        start_index = (index * self.batch_size) + 1\n",
    "        end_index = ((index + 1) * self.batch_size) + 1\n",
    "        \n",
    "        X_context_batch = vectors.query([word_tokenize(q) for q in self.X[0][start_index:end_index]], pad_to_length = self.MAX_CONTEXT_LENGTH)\n",
    "        X_question_batch = vectors.query([word_tokenize(q) for q in self.X[1][start_index:end_index]], pad_to_length = self.MAX_QUESTION_LENGTH)\n",
    "\n",
    "        Y_start_batch = MagnitudeUtils.to_categorical(self.Y[0][start_index:end_index], self.MAX_CONTEXT_LENGTH)\n",
    "        Y_end_batch = MagnitudeUtils.to_categorical(self.Y[1][start_index:end_index], self.MAX_CONTEXT_LENGTH)\n",
    "        \n",
    "        return ((X_context_batch, X_question_batch), (Y_start_batch, Y_end_batch))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = BatchGenerator(x_train, y_train, BATCH_SIZE, MAX_CONTEXT_LENGTH, MAX_QUESTION_LENGTH, TRAIN_NUM_SAMPLES)\n",
    "dev_batch = BatchGenerator(x_dev, y_dev, BATCH_SIZE, MAX_CONTEXT_LENGTH, MAX_QUESTION_LENGTH, DEV_NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_end_prob(inputs):\n",
    "    encoded_context, merged_context, modeled_context, span_begin_probabilities = inputs\n",
    "    weighted_sum = K.sum(K.expand_dims(span_begin_probabilities, axis=-1) * modeled_context, -2)\n",
    "    passage_weighted_by_predicted_span = K.expand_dims(weighted_sum, axis=1)\n",
    "    tile_shape = K.concatenate([[1], [K.shape(encoded_context)[1]], [1]], axis=0)\n",
    "    passage_weighted_by_predicted_span = K.tile(passage_weighted_by_predicted_span, tile_shape)\n",
    "    multiply1 = modeled_context * passage_weighted_by_predicted_span\n",
    "    span_end_representation = K.concatenate(\n",
    "            [merged_context, modeled_context, passage_weighted_by_predicted_span, multiply1])\n",
    "\n",
    "    return span_end_representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_LENGTH = 300\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Concatenate, TimeDistributed, Dense, Softmax, Flatten, Lambda, Multiply, Add, Dropout\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.optimizers import Adam, Adadelta\n",
    "from tensorflow.keras.activations import linear\n",
    "from layers import Similarity, C2QAttention, Q2CAttention, MergedContext, SpanBegin, SpanEnd, Highway\n",
    "\n",
    "######## INPUT LAYER #########\n",
    "context_input = Input(shape = (MAX_CONTEXT_LENGTH, EMBED_LENGTH), dtype = 'float32', name = 'context_input')\n",
    "question_input = Input(shape = (MAX_QUESTION_LENGTH, EMBED_LENGTH), dtype = 'float32', name = 'question_input')\n",
    "\n",
    "\n",
    "####### HIGHWAY LAYER #########\n",
    "\n",
    "highway_layer = Highway(name='highway_1')\n",
    "                        \n",
    "question_layer = TimeDistributed(highway_layer, name='highway_qtd')\n",
    "question_embedding = question_layer(question_input)\n",
    "                        \n",
    "passage_layer = TimeDistributed(highway_layer, name='highway__ptd')\n",
    "context_embedding = passage_layer(context_input)\n",
    "\n",
    "\n",
    "######## CONTEXTUAL EMBEDDING LAYER ########\n",
    "encoder_layer = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout = DROPOUT_RATE), name='bidirectional_encoder')\n",
    "encoded_question = encoder_layer(question_embedding)\n",
    "encoded_context = encoder_layer(context_embedding)\n",
    "\n",
    "\n",
    "######## SIMILARITY LAYER ########\n",
    "similarity_matrix = Similarity(name='similarity_layer')([encoded_context, encoded_question])\n",
    "\n",
    "####### ATTENTION LAYER #########\n",
    "context_to_query_attention = C2QAttention(name='context_to_query_attention')([\n",
    "            similarity_matrix, encoded_question])\n",
    "query_to_context_attention = Q2CAttention(name='query_to_context_attention')([\n",
    "            similarity_matrix, encoded_context])\n",
    "\n",
    "###### MERGE ATTENTIONS ########\n",
    "merged_context = MergedContext(name='merged_context')(\n",
    "            [encoded_context, context_to_query_attention, query_to_context_attention])\n",
    "\n",
    "###### MODELLING LAYER #########\n",
    "modeled_context = Bidirectional(LSTM(64,return_sequences=True, recurrent_dropout = DROPOUT_RATE), name='decoder')(merged_context)\n",
    "\n",
    "###### OUTPUT LAYER SPAN BEGIN#########\n",
    "span_begin_concat = K.concatenate([merged_context, modeled_context], axis = -1)\n",
    "span_begin_weights = TimeDistributed(Dense(1), name = 'Dense_span_begin')(span_begin_concat)\n",
    "span_begin_weights = Dropout(0.2)(span_begin_weights)\n",
    "span_begin_probabilities = Softmax(name = 'span-begin-output')(K.squeeze(span_begin_weights, axis=-1))\n",
    "\n",
    "span_end_representation = Lambda(prepare_for_end_prob)((encoded_context, merged_context, modeled_context, span_begin_probabilities))\n",
    "span_end_representation = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout = DROPOUT_RATE), name='output_end_prob_decoder')(span_end_representation)\n",
    "\n",
    "span_end_input = K.concatenate([merged_context, span_end_representation])\n",
    "span_end_weights = TimeDistributed(Dense(1), name = 'Dense_span_end')(span_end_input)\n",
    "span_end_weights = Dropout(0.2)(span_end_weights)\n",
    "span_end_probabilities = Softmax(name = 'span-end-output')(K.squeeze(span_end_weights, axis=-1))\n",
    "\n",
    "\n",
    "\n",
    "model = Model([context_input, question_input], [span_begin_probabilities, span_end_probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_input (InputLayer)      [(None, 249, 300)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_input (InputLayer)     [(None, 15, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "highway__ptd (TimeDistributed)  (None, 249, 300)     180600      context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "highway_qtd (TimeDistributed)   (None, 15, 300)      180600      question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_encoder (Bidirect multiple             186880      highway_qtd[0][0]                \n",
      "                                                                 highway__ptd[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "similarity_layer (Similarity)   (None, 249, 15)      385         bidirectional_encoder[1][0]      \n",
      "                                                                 bidirectional_encoder[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "context_to_query_attention (C2Q (None, 249, 128)     0           similarity_layer[0][0]           \n",
      "                                                                 bidirectional_encoder[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "query_to_context_attention (Q2C (None, None, 128)    0           similarity_layer[0][0]           \n",
      "                                                                 bidirectional_encoder[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "merged_context (MergedContext)  (None, 249, 512)     0           bidirectional_encoder[1][0]      \n",
      "                                                                 context_to_query_attention[0][0] \n",
      "                                                                 query_to_context_attention[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Bidirectional)         (None, 249, 128)     295424      merged_context[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 249, 640)]   0           merged_context[0][0]             \n",
      "                                                                 decoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense_span_begin (TimeDistribut (None, 249, 1)       641         tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 249, 1)       0           Dense_span_begin[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 249)]        0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "span-begin-output (Softmax)     (None, 249)          0           tf_op_layer_Squeeze[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 249, 896)     0           bidirectional_encoder[1][0]      \n",
      "                                                                 merged_context[0][0]             \n",
      "                                                                 decoder[0][0]                    \n",
      "                                                                 span-begin-output[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_end_prob_decoder (Bidire (None, 249, 128)     492032      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 249, 640)]   0           merged_context[0][0]             \n",
      "                                                                 output_end_prob_decoder[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Dense_span_end (TimeDistributed (None, 249, 1)       641         tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 249, 1)       0           Dense_span_end[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_1 (TensorFl [(None, 249)]        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "span-end-output (Softmax)       (None, 249)          0           tf_op_layer_Squeeze_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,156,603\n",
      "Trainable params: 1,156,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, Adadelta\n",
    "\n",
    "model.compile(optimizer = Adadelta(0.5), loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVLogger, Model Checkpoint(Test save model), 2-highway, Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('./saved_models/bidaf-weights-best.h5', save_best_only = True, save_weights_only = True, mode = 'min', monitor = 'val_loss', verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4/5 [=======================>......] - ETA: 4s - loss: 11.0508 - span-begin-output_loss: 5.5285 - span-end-output_loss: 5.5223 - span-begin-output_accuracy: 0.0000e+00 - span-end-output_accuracy: 0.0000e+00 \n",
      "Epoch 00001: val_loss improved from inf to 11.02727, saving model to ./saved_models/bidaf-weights-best.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './saved_models/bidaf-weights-best.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5a363bf5ebdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     callbacks = [checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    963\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_worker_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './saved_models/bidaf-weights-best.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator = train_batch,\n",
    "                    steps_per_epoch = num_batches_per_epoch_train, \n",
    "                    epochs = 2, \n",
    "                    validation_data = dev_batch, \n",
    "                    validation_steps = num_batches_per_epoch_dev,\n",
    "                    workers = 2,\n",
    "                    use_multiprocessing = True,\n",
    "                    callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
