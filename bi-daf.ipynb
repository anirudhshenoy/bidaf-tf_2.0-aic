{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit : https://github.com/ParikhKadam/bidaf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to : https://www.kaggle.com/sanjay11100/squad-stanford-q-a-json-to-pandas-dataframe\n",
    "# Modified to include first answer_start and text for dev set\n",
    "\n",
    "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    js['q_idx'] = ndx\n",
    "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main\n",
    "\n",
    "\n",
    "def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "#     js['q_idx'] = ndx\n",
    "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    answer_start = []\n",
    "    answer_text = []\n",
    "\n",
    "    for answers in tqdm_notebook(main['answers'].values):\n",
    "        answer_start.append(answers[0]['answer_start'])\n",
    "        answer_text.append(answers[0]['text'])\n",
    "\n",
    "    main['answer_start'] = answer_start\n",
    "    main['text'] = answer_text\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "processing...\n",
      "shape of the dataframe is (87599, 6)\n",
      "Done\n",
      "Reading the json file\n",
      "processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc28566983064ad699bbd9b0d7ffdd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10570), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape of the dataframe is (10570, 7)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "df_train = squad_json_to_dataframe_train('./data/train-v1.1.json')\n",
    "df_dev = squad_json_to_dataframe_dev('./data/dev-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NUM_SAMPLES = 10 #df_train.shape[0]\n",
    "DEV_NUM_SAMPLES = 10 #df_dev.shape[0]\n",
    "\n",
    "\n",
    "df_train = df_train[:TRAIN_NUM_SAMPLES]\n",
    "df_dev = df_dev[:DEV_NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4377e38535640469f9ec0b7ae8c3d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f020a0539624ac6b120c1e44885ca34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "\n",
    "answer_start = []\n",
    "answer_end = []\n",
    "for i in tqdm_notebook(range(df_train.shape[0])): \n",
    "    context_split = word_tokenize(df_train.context.values[i][:df_train.answer_start.values[i]])\n",
    "    answer_start.append(len(context_split))\n",
    "    answer_end.append(len(context_split) + len(word_tokenize(df_train.text.values[i])) -1)\n",
    "df_train['answer_end'] = answer_end\n",
    "df_train['answer_start'] = answer_start\n",
    "\n",
    "answer_start = []\n",
    "answer_end = []\n",
    "for i in tqdm_notebook(range(df_dev.shape[0])): \n",
    "    context_split = word_tokenize(df_dev.context.values[i][:df_dev.answer_start.values[i]])\n",
    "    answer_start.append(len(context_split))\n",
    "    answer_end.append(len(context_split) + len(word_tokenize(df_dev.text.values[i])) -1)\n",
    "df_dev['answer_end'] = answer_end\n",
    "df_dev['answer_start'] = answer_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymagnitude import MagnitudeUtils, Magnitude\n",
    "#from scripts import MagnitudeVectors\n",
    "\n",
    "#vectors = MagnitudeVectors(50).load_vectors()\n",
    "vectors = Magnitude('./data/magnitude/glove.6B.100d.magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43865c01ed2e44489b1bd737cba8d8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2253e0d3f6b644c7833b0b6938a67de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69aa8948877048949c47fda1bc5efb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0707f683ab4e798fb3acaa7e6f8d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.question = [word_tokenize(q) for q in tqdm_notebook(df_train.question.values)]\n",
    "df_train.context = [word_tokenize(q) for q in tqdm_notebook(df_train.context.values)]\n",
    "\n",
    "df_dev.question = [word_tokenize(q) for q in tqdm_notebook(df_dev.question.values)]\n",
    "df_dev.context = [word_tokenize(q) for q in tqdm_notebook(df_dev.context.values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q_train = max([len(t) for t in df_train.question.values])\n",
    "max_c_train = max([len(t) for t in df_train.context.values])\n",
    "\n",
    "max_q_dev = max([len(t) for t in df_dev.question.values])\n",
    "max_c_dev = max([len(t) for t in df_dev.context.values])\n",
    "\n",
    "\n",
    "MAX_QUESTION_LENGTH = max(max_q_train, max_q_dev)\n",
    "MAX_CONTEXT_LENGTH = max(max_c_train, max_c_dev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train.answer_start = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_train.answer_start.values)]\n",
    "df_train.answer_end = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_train.answer_end.values)]\n",
    "\n",
    "df_dev.answer_start = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_dev.answer_start.values)]\n",
    "df_dev.answer_end = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_dev.answer_end.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.answer_start.values, df_train.answer_end.values\n",
    "x_train = df_train.context.values, df_train.question.values\n",
    "\n",
    "y_dev = df_dev.answer_start.values, df_dev.answer_end.values\n",
    "x_dev = df_dev.context.values, df_dev.question.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "num_batches_per_epoch_train = int(np.ceil(TRAIN_NUM_SAMPLES /float(BATCH_SIZE)))\n",
    "num_batches_per_epoch_dev = int(np.ceil(DEV_NUM_SAMPLES /float(BATCH_SIZE))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches_per_epoch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen():\n",
    "    for i in range(TRAIN_NUM_SAMPLES):\n",
    "        if not (i%BATCH_SIZE):\n",
    "            context_pad_length = max([len(t) for t in x_train[0][i:i+BATCH_SIZE]])\n",
    "            question_pad_length = max([len(t) for t in x_train[1][i:i+BATCH_SIZE]])\n",
    "\n",
    "        X_context_batch = vectors.query(x_train[0][i], pad_to_length = context_pad_length)\n",
    "        X_question_batch = vectors.query(x_train[1][i], pad_to_length = question_pad_length)\n",
    "\n",
    "        Y_start_batch = tf.keras.utils.to_categorical(y_train[0][i],context_pad_length)\n",
    "        Y_end_batch = tf.keras.utils.to_categorical(y_train[1][i],context_pad_length)\n",
    "        \n",
    "        yield ((tf.constant(X_context_batch), tf.constant(X_question_batch)), (tf.constant(Y_start_batch), tf.constant(Y_end_batch)))\n",
    "        \n",
    "def dev_gen():\n",
    "    for i in range(DEV_NUM_SAMPLES):\n",
    "        if not (i%BATCH_SIZE):\n",
    "            context_pad_length = max([len(t) for t in x_dev[0][i:i+BATCH_SIZE]])\n",
    "            question_pad_length = max([len(t) for t in x_dev[1][i:i+BATCH_SIZE]])\n",
    "        X_context_batch = vectors.query(x_dev[0][i], pad_to_length = context_pad_length)\n",
    "        X_question_batch = vectors.query(x_dev[1][i], pad_to_length = question_pad_length)\n",
    "\n",
    "        Y_start_batch = tf.keras.utils.to_categorical(y_dev[0][i],context_pad_length)\n",
    "        Y_end_batch =tf.keras.utils.to_categorical(y_dev[1][i],context_pad_length)\n",
    "        \n",
    "        yield ((tf.constant(X_context_batch), tf.constant(X_question_batch)), (tf.constant(Y_start_batch), tf.constant(Y_end_batch)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_generator(train_gen, ((tf.float32, tf.float32), (tf.float32, tf.float32))).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dataset_dev = tf.data.Dataset.from_generator(dev_gen, ((tf.float32, tf.float32), (tf.float32, tf.float32))).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_end_prob(inputs):\n",
    "    encoded_context, merged_context, modeled_context, span_begin_probabilities = inputs\n",
    "    weighted_sum = K.sum(K.expand_dims(span_begin_probabilities, axis=-1) * modeled_context, -2)\n",
    "    passage_weighted_by_predicted_span = K.expand_dims(weighted_sum, axis=1)\n",
    "    tile_shape = K.concatenate([[1], [K.shape(encoded_context)[1]], [1]], axis=0)\n",
    "    passage_weighted_by_predicted_span = K.tile(passage_weighted_by_predicted_span, tile_shape)\n",
    "    multiply1 = modeled_context * passage_weighted_by_predicted_span\n",
    "    span_end_representation = K.concatenate(\n",
    "            [merged_context, modeled_context, passage_weighted_by_predicted_span, multiply1])\n",
    "\n",
    "    return span_end_representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_LENGTH = 100\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Concatenate, TimeDistributed, Dense, Softmax, Flatten, Lambda, Multiply, Add, Dropout\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.optimizers import Adam, Adadelta\n",
    "from tensorflow.keras.activations import linear\n",
    "from layers import Similarity, C2QAttention, Q2CAttention, MergedContext, SpanBegin, SpanEnd, Highway\n",
    "\n",
    "######## INPUT LAYER #########\n",
    "context_input = Input(shape = (None, EMBED_LENGTH), dtype = 'float32', name = 'context_input')\n",
    "question_input = Input(shape = (None, EMBED_LENGTH), dtype = 'float32', name = 'question_input')\n",
    "\n",
    "\n",
    "####### HIGHWAY LAYER #########\n",
    "\n",
    "highway_layer = Highway(name='highway_1')\n",
    "                        \n",
    "question_layer = TimeDistributed(highway_layer, name='highway_qtd')\n",
    "question_embedding = question_layer(question_input)\n",
    "                        \n",
    "passage_layer = TimeDistributed(highway_layer, name='highway__ptd')\n",
    "context_embedding = passage_layer(context_input)\n",
    "\n",
    "\n",
    "######## CONTEXTUAL EMBEDDING LAYER ########\n",
    "encoder_layer = Bidirectional(LSTM(64, return_sequences=True), name='bidirectional_encoder')\n",
    "encoded_question = encoder_layer(question_embedding)\n",
    "encoded_context = encoder_layer(context_embedding)\n",
    "\n",
    "\n",
    "######## SIMILARITY LAYER ########\n",
    "similarity_matrix = Similarity(name='similarity_layer')([encoded_context, encoded_question])\n",
    "\n",
    "####### ATTENTION LAYER #########\n",
    "context_to_query_attention = C2QAttention(name='context_to_query_attention')([\n",
    "            similarity_matrix, encoded_question])\n",
    "query_to_context_attention = Q2CAttention(name='query_to_context_attention')([\n",
    "            similarity_matrix, encoded_context])\n",
    "\n",
    "###### MERGE ATTENTIONS ########\n",
    "merged_context = MergedContext(name='merged_context')(\n",
    "            [encoded_context, context_to_query_attention, query_to_context_attention])\n",
    "\n",
    "###### MODELLING LAYER #########\n",
    "modeled_context = Bidirectional(LSTM(64,return_sequences=True), name='decoder')(merged_context)\n",
    "\n",
    "###### OUTPUT LAYER SPAN BEGIN#########\n",
    "span_begin_concat = K.concatenate([merged_context, modeled_context], axis = -1)\n",
    "span_begin_weights = TimeDistributed(Dense(1), name = 'Dense_span_begin')(span_begin_concat)\n",
    "#span_begin_weights = Dropout(0.2)(span_begin_weights)\n",
    "span_begin_probabilities = Softmax(name = 'span-begin-output')(K.squeeze(span_begin_weights, axis=-1))\n",
    "\n",
    "span_end_representation = Lambda(prepare_for_end_prob)((encoded_context, merged_context, modeled_context, span_begin_probabilities))\n",
    "span_end_representation = Bidirectional(LSTM(64, return_sequences=True), name='output_end_prob_decoder')(span_end_representation)\n",
    "\n",
    "span_end_input = K.concatenate([merged_context, span_end_representation])\n",
    "span_end_weights = TimeDistributed(Dense(1), name = 'Dense_span_end')(span_end_input)\n",
    "#span_end_weights = Dropout(0.2)(span_end_weights)\n",
    "span_end_probabilities = Softmax(name = 'span-end-output')(K.squeeze(span_end_weights, axis=-1))\n",
    "\n",
    "\n",
    "\n",
    "model = Model([context_input, question_input], [span_begin_probabilities, span_end_probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_input (InputLayer)      [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_input (InputLayer)     [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "highway__ptd (TimeDistributed)  (None, None, 100)    20200       context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "highway_qtd (TimeDistributed)   (None, None, 100)    20200       question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_encoder (Bidirect (None, None, 128)    84480       highway_qtd[0][0]                \n",
      "                                                                 highway__ptd[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "similarity_layer (Similarity)   (None, None, None)   385         bidirectional_encoder[1][0]      \n",
      "                                                                 bidirectional_encoder[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "context_to_query_attention (C2Q (None, None, 128)    0           similarity_layer[0][0]           \n",
      "                                                                 bidirectional_encoder[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "query_to_context_attention (Q2C (None, None, 128)    0           similarity_layer[0][0]           \n",
      "                                                                 bidirectional_encoder[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "merged_context (MergedContext)  (None, None, 512)    0           bidirectional_encoder[1][0]      \n",
      "                                                                 context_to_query_attention[0][0] \n",
      "                                                                 query_to_context_attention[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Bidirectional)         (None, None, 128)    295424      merged_context[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, None, 640)]  0           merged_context[0][0]             \n",
      "                                                                 decoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense_span_begin (TimeDistribut (None, None, 1)      641         tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, None)]       0           Dense_span_begin[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "span-begin-output (Softmax)     (None, None)         0           tf_op_layer_Squeeze[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, 896)    0           bidirectional_encoder[1][0]      \n",
      "                                                                 merged_context[0][0]             \n",
      "                                                                 decoder[0][0]                    \n",
      "                                                                 span-begin-output[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_end_prob_decoder (Bidire (None, None, 128)    492032      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, None, 640)]  0           merged_context[0][0]             \n",
      "                                                                 output_end_prob_decoder[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Dense_span_end (TimeDistributed (None, None, 1)      641         tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_1 (TensorFl [(None, None)]       0           Dense_span_end[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "span-end-output (Softmax)       (None, None)         0           tf_op_layer_Squeeze_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 893,803\n",
      "Trainable params: 893,803\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, Adadelta\n",
    "\n",
    "model.compile(optimizer = Adadelta(0.5), loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVLogger, Model Checkpoint(Test save model), 2-highway, Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('./logs/saved_models/bidaf-weights-best.h5', save_best_only = True, save_weights_only = True, mode = 'min', monitor = 'val_loss', verbose = 1)\n",
    "logger = CSVLogger('./logs/training.log', append = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches_per_epoch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4/5 [=======================>......] - ETA: 1s - loss: 10.4478 - span-begin-output_loss: 5.2217 - span-end-output_loss: 5.2261 - span-begin-output_accuracy: 0.0000e+00 - span-end-output_accuracy: 0.0000e+00\n",
      "Epoch 00001: val_loss improved from inf to 9.96631, saving model to ./logs/saved_models/bidaf-weights-best.h5\n",
      "5/5 [==============================] - 10s 2s/step - loss: 10.5643 - span-begin-output_loss: 5.2797 - span-end-output_loss: 5.2847 - span-begin-output_accuracy: 0.0000e+00 - span-end-output_accuracy: 0.0000e+00 - val_loss: 9.9663 - val_span-begin-output_loss: 4.9833 - val_span-end-output_loss: 4.9830 - val_span-begin-output_accuracy: 0.0000e+00 - val_span-end-output_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "4/5 [=======================>......] - ETA: 1s - loss: 10.4257 - span-begin-output_loss: 5.2112 - span-end-output_loss: 5.2145 - span-begin-output_accuracy: 0.1250 - span-end-output_accuracy: 0.0000e+00 \n",
      "Epoch 00002: val_loss improved from 9.96631 to 9.95804, saving model to ./logs/saved_models/bidaf-weights-best.h5\n",
      "5/5 [==============================] - 9s 2s/step - loss: 10.5434 - span-begin-output_loss: 5.2698 - span-end-output_loss: 5.2736 - span-begin-output_accuracy: 0.1000 - span-end-output_accuracy: 0.0000e+00 - val_loss: 9.9580 - val_span-begin-output_loss: 4.9798 - val_span-end-output_loss: 4.9783 - val_span-begin-output_accuracy: 0.0000e+00 - val_span-end-output_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator = dataset_train,\n",
    "                    steps_per_epoch = 5, \n",
    "                    epochs = 2, \n",
    "                    validation_data = dataset_dev, \n",
    "                    validation_steps = num_batches_per_epoch_dev,\n",
    "                    #workers = 8,\n",
    "                    #use_multiprocessing = True,\n",
    "                    #shuffle = True,\n",
    "                    callbacks = [checkpoint, logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_gen().__next__()[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[<tf.Tensor: id=206878, shape=(249, 300), dtype=float32, numpy=\narray([[ 0.0579464, -0.018288 ,  0.0553442, ...,  0.0135293,  0.0279059,\n        -0.0068382],\n       [-0.0162427,  0.131909 , -0...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-b6bb38b63a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.predict(np.array(train_gen().__next__()[0]), verbose = 1)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/anirudh/.local/lib/python3.7/site-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    529\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       raise ValueError('Error when checking model ' + exception_prefix +\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[<tf.Tensor: id=206878, shape=(249, 300), dtype=float32, numpy=\narray([[ 0.0579464, -0.018288 ,  0.0553442, ...,  0.0135293,  0.0279059,\n        -0.0068382],\n       [-0.0162427,  0.131909 , -0..."
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model.predict(np.array(train_gen().__next__()[0]), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
