{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit : https://github.com/ParikhKadam/bidaf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to : https://www.kaggle.com/sanjay11100/squad-stanford-q-a-json-to-pandas-dataframe\n",
    "# Modified to include first answer_start and text for dev set\n",
    "\n",
    "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    js['q_idx'] = ndx\n",
    "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main\n",
    "\n",
    "\n",
    "def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "#     js['q_idx'] = ndx\n",
    "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    answer_start = []\n",
    "    answer_text = []\n",
    "\n",
    "    for answers in tqdm_notebook(main['answers'].values):\n",
    "        answer_start.append(answers[0]['answer_start'])\n",
    "        answer_text.append(answers[0]['text'])\n",
    "\n",
    "    main['answer_start'] = answer_start\n",
    "    main['text'] = answer_text\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "processing...\n",
      "shape of the dataframe is (87599, 6)\n",
      "Done\n",
      "Reading the json file\n",
      "processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf3336280c84e3faa7de7c22dadd7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10570), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape of the dataframe is (10570, 7)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "df_train = squad_json_to_dataframe_train('./data/train-v1.1.json')\n",
    "df_dev = squad_json_to_dataframe_dev('./data/dev-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NUM_SAMPLES = 100 #df_train.shape[0]\n",
    "DEV_NUM_SAMPLES = 100 #df_dev.shape[0]\n",
    "\n",
    "\n",
    "df_train = df_train[:TRAIN_NUM_SAMPLES]\n",
    "df_dev = df_dev[:DEV_NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a604479a3bc4455919ca58517f99468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9113c64039f7493ebe424f839afc450a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "\n",
    "answer_start = []\n",
    "answer_end = []\n",
    "for i in tqdm_notebook(range(df_train.shape[0])): \n",
    "    context_split = word_tokenize(df_train.context.values[i][:df_train.answer_start.values[i]])\n",
    "    answer_start.append(len(context_split))\n",
    "    answer_end.append(len(context_split) + len(word_tokenize(df_train.text.values[i])))\n",
    "df_train['answer_end'] = answer_end\n",
    "df_train['answer_start'] = answer_start\n",
    "\n",
    "answer_start = []\n",
    "answer_end = []\n",
    "for i in tqdm_notebook(range(df_dev.shape[0])): \n",
    "    context_split = word_tokenize(df_dev.context.values[i][:df_dev.answer_start.values[i]])\n",
    "    answer_start.append(len(context_split))\n",
    "    answer_end.append(len(context_split) + len(word_tokenize(df_dev.text.values[i])))\n",
    "df_dev['answer_end'] = answer_end\n",
    "df_dev['answer_start'] = answer_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will download magnitude files from the server if they aren't avaialble locally.. So, grab a cup of coffee while the downloading is under progress..\n"
     ]
    }
   ],
   "source": [
    "from pymagnitude import MagnitudeUtils\n",
    "from scripts import MagnitudeVectors\n",
    "\n",
    "vectors = MagnitudeVectors(50).load_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0e9ed3bd364253824bd40989d3ea54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5000361be44afd8e3c24dbe1d6ff7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2758e76465d47eb8de0b2c8dbf98121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd37acd75474a898e6668544b978ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.question = [word_tokenize(q) for q in tqdm_notebook(df_train.question.values)]\n",
    "df_train.context = [word_tokenize(q) for q in tqdm_notebook(df_train.context.values)]\n",
    "\n",
    "df_dev.question = [word_tokenize(q) for q in tqdm_notebook(df_dev.question.values)]\n",
    "df_dev.context = [word_tokenize(q) for q in tqdm_notebook(df_dev.context.values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q_train = max([len(t) for t in df_train.question.values])\n",
    "max_c_train = max([len(t) for t in df_train.context.values])\n",
    "\n",
    "max_q_dev = max([len(t) for t in df_dev.question.values])\n",
    "max_c_dev = max([len(t) for t in df_dev.context.values])\n",
    "\n",
    "\n",
    "MAX_QUESTION_LENGTH = max(max_q_train, max_q_dev)\n",
    "MAX_CONTEXT_LENGTH = max(max_c_train, max_c_dev)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train.answer_start = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_train.answer_start.values)]\n",
    "df_train.answer_end = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_train.answer_end.values)]\n",
    "\n",
    "df_dev.answer_start = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_dev.answer_start.values)]\n",
    "df_dev.answer_end = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_dev.answer_end.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.answer_start.values, df_train.answer_end.values\n",
    "x_train = df_train.context.values, df_train.question.values\n",
    "\n",
    "y_dev = df_dev.answer_start.values, df_dev.answer_end.values\n",
    "x_dev = df_dev.context.values, df_dev.question.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "num_batches_per_epoch_train = int(np.ceil(TRAIN_NUM_SAMPLES/float(BATCH_SIZE)))\n",
    "num_batches_per_epoch_dev = int(np.ceil(DEV_NUM_SAMPLES/float(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen():\n",
    "    for i in range(TRAIN_NUM_SAMPLES):\n",
    "        if not (i%BATCH_SIZE):\n",
    "            context_pad_length = max([len(t) for t in x_train[0][i:i+BATCH_SIZE]])\n",
    "            question_pad_length = max([len(t) for t in x_train[1][i:i+BATCH_SIZE]])\n",
    "\n",
    "        X_context_batch = vectors.query(x_train[0][i], pad_to_length = context_pad_length)\n",
    "        X_question_batch = vectors.query(x_train[1][i], pad_to_length = question_pad_length)\n",
    "\n",
    "        Y_start_batch = tf.keras.utils.to_categorical(y_train[0][i],context_pad_length)\n",
    "        Y_end_batch =tf.keras.utils.to_categorical(y_train[1][i],context_pad_length)\n",
    "        \n",
    "        yield ((tf.constant(X_context_batch), tf.constant(X_question_batch)), (tf.constant(Y_start_batch), tf.constant(Y_end_batch)))\n",
    "        \n",
    "def dev_gen():\n",
    "    for i in range(DEV_NUM_SAMPLES):\n",
    "        X_context_batch = vectors.query(x_dev[0][i], pad_to_length = MAX_CONTEXT_LENGTH)\n",
    "        X_question_batch = vectors.query(x_dev[1][i], pad_to_length = MAX_QUESTION_LENGTH)\n",
    "\n",
    "        Y_start_batch = y_dev[0][i]\n",
    "        Y_end_batch = y_dev[1][i]\n",
    "        \n",
    "        yield ((tf.constant(X_context_batch), tf.constant(X_question_batch)), (tf.constant(Y_start_batch), tf.constant(Y_end_batch)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_generator(train_gen, ((tf.float32, tf.float32), (tf.float32, tf.float32))).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dataset_dev = tf.data.Dataset.from_generator(dev_gen, ((tf.float32, tf.float32), (tf.float32, tf.float32))).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: id=499718, shape=(10, 249, 300), dtype=float32, numpy=\n",
      "array([[[ 0.0579464, -0.018288 ,  0.0553442, ...,  0.0135293,\n",
      "          0.0279059, -0.0068382],\n",
      "        [-0.0162427,  0.131909 , -0.0294167, ..., -0.0376468,\n",
      "         -0.0742847, -0.012933 ],\n",
      "        [ 0.0577659, -0.0131717, -0.0400055, ...,  0.0276365,\n",
      "         -0.038895 ,  0.028093 ],\n",
      "        ...,\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ]],\n",
      "\n",
      "       [[ 0.0579464, -0.018288 ,  0.0553442, ...,  0.0135293,\n",
      "          0.0279059, -0.0068382],\n",
      "        [-0.0162427,  0.131909 , -0.0294167, ..., -0.0376468,\n",
      "         -0.0742847, -0.012933 ],\n",
      "        [ 0.0577659, -0.0131717, -0.0400055, ...,  0.0276365,\n",
      "         -0.038895 ,  0.028093 ],\n",
      "        ...,\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ]],\n",
      "\n",
      "       [[ 0.0579464, -0.018288 ,  0.0553442, ...,  0.0135293,\n",
      "          0.0279059, -0.0068382],\n",
      "        [-0.0162427,  0.131909 , -0.0294167, ..., -0.0376468,\n",
      "         -0.0742847, -0.012933 ],\n",
      "        [ 0.0577659, -0.0131717, -0.0400055, ...,  0.0276365,\n",
      "         -0.038895 ,  0.028093 ],\n",
      "        ...,\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.0212525, -0.0022433, -0.0273397, ..., -0.0566133,\n",
      "         -0.0097162, -0.0451662],\n",
      "        [-0.0602788,  0.09807  ,  0.0216875, ...,  0.0446373,\n",
      "         -0.0079292,  0.0961388],\n",
      "        [-0.0374794,  0.0107769, -0.0184836, ..., -0.0967309,\n",
      "          0.0518288, -0.0217051],\n",
      "        ...,\n",
      "        [ 0.0496613,  0.0459276,  0.0118493, ...,  0.0136374,\n",
      "         -0.0080807,  0.0553111],\n",
      "        [ 0.0710144,  0.0460947,  0.0047089, ..., -0.0182301,\n",
      "         -0.0002959, -0.0601388],\n",
      "        [ 0.0024335,  0.0420773, -0.0255047, ...,  0.0281266,\n",
      "         -0.0730975, -0.007097 ]],\n",
      "\n",
      "       [[-0.0212525, -0.0022433, -0.0273397, ..., -0.0566133,\n",
      "         -0.0097162, -0.0451662],\n",
      "        [-0.0602788,  0.09807  ,  0.0216875, ...,  0.0446373,\n",
      "         -0.0079292,  0.0961388],\n",
      "        [-0.0374794,  0.0107769, -0.0184836, ..., -0.0967309,\n",
      "          0.0518288, -0.0217051],\n",
      "        ...,\n",
      "        [ 0.0496613,  0.0459276,  0.0118493, ...,  0.0136374,\n",
      "         -0.0080807,  0.0553111],\n",
      "        [ 0.0710144,  0.0460947,  0.0047089, ..., -0.0182301,\n",
      "         -0.0002959, -0.0601388],\n",
      "        [ 0.0024335,  0.0420773, -0.0255047, ...,  0.0281266,\n",
      "         -0.0730975, -0.007097 ]],\n",
      "\n",
      "       [[-0.0212525, -0.0022433, -0.0273397, ..., -0.0566133,\n",
      "         -0.0097162, -0.0451662],\n",
      "        [-0.0602788,  0.09807  ,  0.0216875, ...,  0.0446373,\n",
      "         -0.0079292,  0.0961388],\n",
      "        [-0.0374794,  0.0107769, -0.0184836, ..., -0.0967309,\n",
      "          0.0518288, -0.0217051],\n",
      "        ...,\n",
      "        [ 0.0496613,  0.0459276,  0.0118493, ...,  0.0136374,\n",
      "         -0.0080807,  0.0553111],\n",
      "        [ 0.0710144,  0.0460947,  0.0047089, ..., -0.0182301,\n",
      "         -0.0002959, -0.0601388],\n",
      "        [ 0.0024335,  0.0420773, -0.0255047, ...,  0.0281266,\n",
      "         -0.0730975, -0.007097 ]]], dtype=float32)>, <tf.Tensor: id=499719, shape=(10, 15, 300), dtype=float32, numpy=\n",
      "array([[[-0.0097565,  0.0343063, -0.006055 , ..., -0.006945 ,\n",
      "         -0.0293492,  0.0332475],\n",
      "        [ 0.0301252,  0.0012558,  0.0129633, ...,  0.0011376,\n",
      "          0.0338616, -0.0666993],\n",
      "        [-0.0130372,  0.0733647, -0.0494283, ...,  0.03653  ,\n",
      "          0.0710125,  0.0275886],\n",
      "        ...,\n",
      "        [ 0.0019372,  0.0702289, -0.0162045, ..., -0.0221669,\n",
      "          0.1018019,  0.0193279],\n",
      "        [-0.0168314,  0.0371278,  0.0211497, ..., -0.0029375,\n",
      "          0.0215237,  0.040013 ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ]],\n",
      "\n",
      "       [[-0.0425703,  0.0757713, -0.0148449, ...,  0.0294512,\n",
      "          0.1033074,  0.0078473],\n",
      "        [-0.0173734,  0.1026521,  0.0004871, ..., -0.043987 ,\n",
      "         -0.053788 , -0.0012305],\n",
      "        [ 0.0175117,  0.0506422,  0.0516043, ...,  0.0283154,\n",
      "         -0.0331829,  0.0520343],\n",
      "        ...,\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ]],\n",
      "\n",
      "       [[-0.0136407,  0.0190495, -0.0507361, ..., -0.0372625,\n",
      "         -0.0071417, -0.0121085],\n",
      "        [ 0.1136426, -0.118268 , -0.0448637, ...,  0.1119917,\n",
      "          0.0646434, -0.0036452],\n",
      "        [ 0.0120966,  0.0437913, -0.0085357, ...,  0.0235218,\n",
      "         -0.033532 , -0.0189004],\n",
      "        ...,\n",
      "        [ 0.0462428,  0.0245075,  0.0188332, ..., -0.0490915,\n",
      "          0.0414728,  0.0022191],\n",
      "        [ 0.0043644,  0.0242283, -0.0116866, ...,  0.0732139,\n",
      "          0.0302373, -0.0151922],\n",
      "        [-0.0168314,  0.0371278,  0.0211497, ..., -0.0029375,\n",
      "          0.0215237,  0.040013 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.0425703,  0.0757713, -0.0148449, ...,  0.0294512,\n",
      "          0.1033074,  0.0078473],\n",
      "        [-0.0173734,  0.1026521,  0.0004871, ..., -0.043987 ,\n",
      "         -0.053788 , -0.0012305],\n",
      "        [ 0.0577659, -0.0131717, -0.0400055, ...,  0.0276365,\n",
      "         -0.038895 ,  0.028093 ],\n",
      "        ...,\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ]],\n",
      "\n",
      "       [[-0.0855537,  0.0615842, -0.0538778, ...,  0.072598 ,\n",
      "          0.0651251,  0.0506426],\n",
      "        [ 0.017146 , -0.0050597, -0.0730719, ..., -0.0382245,\n",
      "          0.0586321, -0.0032752],\n",
      "        [-0.0448832, -0.0064602,  0.0009713, ...,  0.0321198,\n",
      "          0.0498648, -0.0141396],\n",
      "        ...,\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ],\n",
      "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
      "          0.       ,  0.       ]],\n",
      "\n",
      "       [[-0.0078475,  0.1097944,  0.080019 , ..., -0.0201584,\n",
      "         -0.0371914, -0.0228637],\n",
      "        [-0.0075057,  0.1056347, -0.0425308, ...,  0.022972 ,\n",
      "          0.0478795,  0.0445363],\n",
      "        [-0.0189233,  0.0338645,  0.0304533, ...,  0.0286961,\n",
      "          0.0250459, -0.0602017],\n",
      "        ...,\n",
      "        [ 0.0716125,  0.0068446, -0.0281634, ...,  0.0464665,\n",
      "          0.0747725, -0.0385519],\n",
      "        [ 0.0958396, -0.0106553, -0.0181907, ...,  0.018732 ,\n",
      "          0.0795325, -0.0552505],\n",
      "        [-0.0168314,  0.0371278,  0.0211497, ..., -0.0029375,\n",
      "          0.0215237,  0.040013 ]]], dtype=float32)>), (<tf.Tensor: id=499720, shape=(10, 249), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: id=499721, shape=(10, 249), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>))\n"
     ]
    }
   ],
   "source": [
    "for t in dataset_train.take(1):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_end_prob(inputs):\n",
    "    encoded_context, merged_context, modeled_context, span_begin_probabilities = inputs\n",
    "    weighted_sum = K.sum(K.expand_dims(span_begin_probabilities, axis=-1) * modeled_context, -2)\n",
    "    passage_weighted_by_predicted_span = K.expand_dims(weighted_sum, axis=1)\n",
    "    tile_shape = K.concatenate([[1], [K.shape(encoded_context)[1]], [1]], axis=0)\n",
    "    passage_weighted_by_predicted_span = K.tile(passage_weighted_by_predicted_span, tile_shape)\n",
    "    multiply1 = modeled_context * passage_weighted_by_predicted_span\n",
    "    span_end_representation = K.concatenate(\n",
    "            [merged_context, modeled_context, passage_weighted_by_predicted_span, multiply1])\n",
    "\n",
    "    return span_end_representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_LENGTH = 300\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Concatenate, TimeDistributed, Dense, Softmax, Flatten, Lambda, Multiply, Add, Dropout\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.optimizers import Adam, Adadelta\n",
    "from tensorflow.keras.activations import linear\n",
    "from layers import Similarity, C2QAttention, Q2CAttention, MergedContext, SpanBegin, SpanEnd, Highway\n",
    "\n",
    "######## INPUT LAYER #########\n",
    "context_input = Input(shape = (None, EMBED_LENGTH), dtype = 'float32', name = 'context_input')\n",
    "question_input = Input(shape = (None, EMBED_LENGTH), dtype = 'float32', name = 'question_input')\n",
    "\n",
    "\n",
    "####### HIGHWAY LAYER #########\n",
    "\n",
    "highway_layer = Highway(name='highway_1')\n",
    "                        \n",
    "question_layer = TimeDistributed(highway_layer, name='highway_qtd')\n",
    "question_embedding = question_layer(question_input)\n",
    "                        \n",
    "passage_layer = TimeDistributed(highway_layer, name='highway__ptd')\n",
    "context_embedding = passage_layer(context_input)\n",
    "\n",
    "\n",
    "######## CONTEXTUAL EMBEDDING LAYER ########\n",
    "encoder_layer = Bidirectional(LSTM(64, return_sequences=True), name='bidirectional_encoder')\n",
    "encoded_question = encoder_layer(question_embedding)\n",
    "encoded_context = encoder_layer(context_embedding)\n",
    "\n",
    "\n",
    "######## SIMILARITY LAYER ########\n",
    "similarity_matrix = Similarity(name='similarity_layer')([encoded_context, encoded_question])\n",
    "\n",
    "####### ATTENTION LAYER #########\n",
    "context_to_query_attention = C2QAttention(name='context_to_query_attention')([\n",
    "            similarity_matrix, encoded_question])\n",
    "query_to_context_attention = Q2CAttention(name='query_to_context_attention')([\n",
    "            similarity_matrix, encoded_context])\n",
    "\n",
    "###### MERGE ATTENTIONS ########\n",
    "merged_context = MergedContext(name='merged_context')(\n",
    "            [encoded_context, context_to_query_attention, query_to_context_attention])\n",
    "\n",
    "###### MODELLING LAYER #########\n",
    "modeled_context = Bidirectional(LSTM(64,return_sequences=True), name='decoder')(merged_context)\n",
    "\n",
    "###### OUTPUT LAYER SPAN BEGIN#########\n",
    "span_begin_concat = K.concatenate([merged_context, modeled_context], axis = -1)\n",
    "span_begin_weights = TimeDistributed(Dense(1), name = 'Dense_span_begin')(span_begin_concat)\n",
    "#span_begin_weights = Dropout(0.2)(span_begin_weights)\n",
    "span_begin_probabilities = Softmax(name = 'span-begin-output')(K.squeeze(span_begin_weights, axis=-1))\n",
    "\n",
    "span_end_representation = Lambda(prepare_for_end_prob)((encoded_context, merged_context, modeled_context, span_begin_probabilities))\n",
    "span_end_representation = Bidirectional(LSTM(64, return_sequences=True), name='output_end_prob_decoder')(span_end_representation)\n",
    "\n",
    "span_end_input = K.concatenate([merged_context, span_end_representation])\n",
    "span_end_weights = TimeDistributed(Dense(1), name = 'Dense_span_end')(span_end_input)\n",
    "#span_end_weights = Dropout(0.2)(span_end_weights)\n",
    "span_end_probabilities = Softmax(name = 'span-end-output')(K.squeeze(span_end_weights, axis=-1))\n",
    "\n",
    "\n",
    "\n",
    "model = Model([context_input, question_input], [span_begin_probabilities, span_end_probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_input (InputLayer)      [(None, None, 300)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_input (InputLayer)     [(None, None, 300)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "highway__ptd (TimeDistributed)  (None, None, 300)    180600      context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "highway_qtd (TimeDistributed)   (None, None, 300)    180600      question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_encoder (Bidirect (None, None, 128)    186880      highway_qtd[0][0]                \n",
      "                                                                 highway__ptd[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "similarity_layer (Similarity)   (None, None, None)   385         bidirectional_encoder[1][0]      \n",
      "                                                                 bidirectional_encoder[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "context_to_query_attention (C2Q (None, None, 128)    0           similarity_layer[0][0]           \n",
      "                                                                 bidirectional_encoder[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "query_to_context_attention (Q2C (None, None, 128)    0           similarity_layer[0][0]           \n",
      "                                                                 bidirectional_encoder[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "merged_context (MergedContext)  (None, None, 512)    0           bidirectional_encoder[1][0]      \n",
      "                                                                 context_to_query_attention[0][0] \n",
      "                                                                 query_to_context_attention[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Bidirectional)         (None, None, 128)    295424      merged_context[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, None, 640)]  0           merged_context[0][0]             \n",
      "                                                                 decoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense_span_begin (TimeDistribut (None, None, 1)      641         tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, None)]       0           Dense_span_begin[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "span-begin-output (Softmax)     (None, None)         0           tf_op_layer_Squeeze[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, 896)    0           bidirectional_encoder[1][0]      \n",
      "                                                                 merged_context[0][0]             \n",
      "                                                                 decoder[0][0]                    \n",
      "                                                                 span-begin-output[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_end_prob_decoder (Bidire (None, None, 128)    492032      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, None, 640)]  0           merged_context[0][0]             \n",
      "                                                                 output_end_prob_decoder[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Dense_span_end (TimeDistributed (None, None, 1)      641         tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_1 (TensorFl [(None, None)]       0           Dense_span_end[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "span-end-output (Softmax)       (None, None)         0           tf_op_layer_Squeeze_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,156,603\n",
      "Trainable params: 1,156,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, Adadelta\n",
    "\n",
    "model.compile(optimizer = Adadelta(0.5), loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVLogger, Model Checkpoint(Test save model), 2-highway, Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('./logs/saved_models/bidaf-weights-best.h5', save_best_only = True, save_weights_only = True, mode = 'min', monitor = 'val_loss', verbose = 1)\n",
    "logger = CSVLogger('./logs/training.log', append = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/50 [====================>.........] - ETA: 26s - loss: 10.0899 - span-begin-output_loss: 5.0473 - span-end-output_loss: 5.0426 - span-begin-output_accuracy: 0.0556 - span-end-output_accuracy: 0.0556"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator = dataset_train,\n",
    "                    steps_per_epoch = num_batches_per_epoch_train, \n",
    "                    epochs = 1, \n",
    "                    validation_data = dataset_dev, \n",
    "                    validation_steps = num_batches_per_epoch_dev,\n",
    "                    #workers = 8,\n",
    "                    #use_multiprocessing = True,\n",
    "                    shuffle = True,\n",
    "                    callbacks = [checkpoint, logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 0s 25ms/step"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b664eab569ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    501\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 408\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;34m\"\"\"A single step of the distributed execution across replicas.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     x, y, sample_weights = _prepare_feed_values(\n\u001b[0;32m---> 66\u001b[0;31m         model, input_iterator, mode)\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Call `Model.{train,test,predict}_on_batch` on every replica passing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# PerReplicas as arguments.  On every replica inside this call, each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_prepare_feed_values\u001b[0;34m(model, inputs, mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mwill\u001b[0m \u001b[0malways\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mwrapped\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \"\"\"\n\u001b[0;32m--> 112\u001b[0;31m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_input_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;31m# When the inputs are dict, then we want to flatten it in the same order as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_get_input_from_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0;31m# Validate that all the elements in x and y are of the same type and shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   dist_utils.validate_distributed_dataset_inputs(\n\u001b[0;32m--> 149\u001b[0;31m       distribution_strategy_context.get_strategy(), x, y, sample_weights)\n\u001b[0m\u001b[1;32m    150\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mvalidate_distributed_dataset_inputs\u001b[0;34m(distribution_strategy, x, y, sample_weights)\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0;31m# If each element of x and y are not tensors, we cannot standardize and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;31m# validate the input and targets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mx_values_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_per_replica_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mvalidate_per_replica_inputs\u001b[0;34m(distribution_strategy, x)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;31m# Validate that the shape and dtype of all the elements in x are the same.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mvalidate_all_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0mvalidate_all_tensor_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mvalidate_all_tensor_shapes\u001b[0;34m(x, x_values)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_all_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m   \u001b[0;31m# Validate that the shape of all the elements in x have the same shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m   \u001b[0mx_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx_shape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \"\"\"\n\u001b[1;32m   1170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"as_list() is not defined on an unknown TensorShape.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "model.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_gen().__next__()[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[<tf.Tensor: id=206878, shape=(249, 300), dtype=float32, numpy=\narray([[ 0.0579464, -0.018288 ,  0.0553442, ...,  0.0135293,  0.0279059,\n        -0.0068382],\n       [-0.0162427,  0.131909 , -0...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-b6bb38b63a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.predict(np.array(train_gen().__next__()[0]), verbose = 1)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/anirudh/.local/lib/python3.7/site-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    529\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       raise ValueError('Error when checking model ' + exception_prefix +\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[<tf.Tensor: id=206878, shape=(249, 300), dtype=float32, numpy=\narray([[ 0.0579464, -0.018288 ,  0.0553442, ...,  0.0135293,  0.0279059,\n        -0.0068382],\n       [-0.0162427,  0.131909 , -0..."
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model.predict(np.array(train_gen().__next__()[0]), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
