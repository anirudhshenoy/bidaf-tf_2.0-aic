{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Lambda\n",
    "import subprocess as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = x_train.shape[0]\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "ds_train = ds_train.map(lambda x, y: (tf.reshape(x, [-1])/255, tf.one_hot(y, num_classes)))\n",
    "ds_train = ds_train.batch(BATCH_SIZE).repeat().prefetch(1)\n",
    "\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "ds_test = ds_test.map(lambda x, y: (tf.reshape(x, [-1])/255, tf.one_hot(y, num_classes)))\n",
    "ds_test = ds_test.batch(BATCH_SIZE).repeat().prefetch(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "memory_usage = []\n",
    "class MemoryCheck(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        mem = sp.check_output('nvidia-smi | grep python', shell=True).split()[-2].decode('utf-8')\n",
    "        memory_usage.append(int(mem[:-3]))\n",
    "        print(' ' + mem)\n",
    "\n",
    "mem_check = MemoryCheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def cube_this(x):\n",
    "    return x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Highway\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation = 'relu', input_shape = (784, )))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Highway())\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "highway (Highway)            (None, 512)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 960,714\n",
      "Trainable params: 960,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('RMSprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 911MiB\n",
      "  1/235 [..............................] - ETA: 57s - loss: 2.3663 - accuracy: 0.0742 911MiB\n",
      "  2/235 [..............................] - ETA: 36s - loss: 2.1958 - accuracy: 0.2148 911MiB\n",
      "  3/235 [..............................] - ETA: 29s - loss: 2.0524 - accuracy: 0.2878 911MiB\n",
      "  4/235 [..............................] - ETA: 25s - loss: 1.9125 - accuracy: 0.3535 911MiB\n",
      "  5/235 [..............................] - ETA: 23s - loss: 1.8717 - accuracy: 0.3758 911MiB\n",
      "  6/235 [..............................] - ETA: 22s - loss: 1.7904 - accuracy: 0.4062 911MiB\n",
      "  7/235 [..............................] - ETA: 21s - loss: 1.6974 - accuracy: 0.4448 911MiB\n",
      "  8/235 [>.............................] - ETA: 20s - loss: 1.6180 - accuracy: 0.4785 911MiB\n",
      "  9/235 [>.............................] - ETA: 19s - loss: 1.5393 - accuracy: 0.5056 911MiB\n",
      " 10/235 [>.............................] - ETA: 19s - loss: 1.4796 - accuracy: 0.5266 911MiB\n",
      " 11/235 [>.............................] - ETA: 18s - loss: 1.4267 - accuracy: 0.5412 911MiB\n",
      " 12/235 [>.............................] - ETA: 18s - loss: 1.3766 - accuracy: 0.5566 911MiB\n",
      " 13/235 [>.............................] - ETA: 18s - loss: 1.3300 - accuracy: 0.5724 911MiB\n",
      " 14/235 [>.............................] - ETA: 18s - loss: 1.2849 - accuracy: 0.5915 911MiB\n",
      " 15/235 [>.............................] - ETA: 17s - loss: 1.2442 - accuracy: 0.6047 911MiB\n",
      " 16/235 [=>............................] - ETA: 17s - loss: 1.2069 - accuracy: 0.6167 911MiB\n",
      " 17/235 [=>............................] - ETA: 17s - loss: 1.1725 - accuracy: 0.6271 911MiB\n",
      " 18/235 [=>............................] - ETA: 16s - loss: 1.1382 - accuracy: 0.6389 911MiB\n",
      " 19/235 [=>............................] - ETA: 16s - loss: 1.1116 - accuracy: 0.6470 911MiB\n",
      " 20/235 [=>............................] - ETA: 16s - loss: 1.0900 - accuracy: 0.6531 911MiB\n",
      " 21/235 [=>............................] - ETA: 16s - loss: 1.0717 - accuracy: 0.6600 911MiB\n",
      " 22/235 [=>............................] - ETA: 16s - loss: 1.0454 - accuracy: 0.6683 911MiB\n",
      " 23/235 [=>............................] - ETA: 16s - loss: 1.0204 - accuracy: 0.6766 911MiB\n",
      " 24/235 [==>...........................] - ETA: 16s - loss: 0.9948 - accuracy: 0.6862 911MiB\n",
      " 25/235 [==>...........................] - ETA: 15s - loss: 0.9779 - accuracy: 0.6911 911MiB\n",
      " 26/235 [==>...........................] - ETA: 15s - loss: 0.9612 - accuracy: 0.6964 911MiB\n",
      " 27/235 [==>...........................] - ETA: 15s - loss: 0.9473 - accuracy: 0.7024 911MiB\n",
      " 28/235 [==>...........................] - ETA: 15s - loss: 0.9346 - accuracy: 0.7068 911MiB\n",
      " 29/235 [==>...........................] - ETA: 15s - loss: 0.9229 - accuracy: 0.7112 911MiB\n",
      " 30/235 [==>...........................] - ETA: 15s - loss: 0.9090 - accuracy: 0.7152 911MiB\n",
      " 31/235 [==>...........................] - ETA: 15s - loss: 0.8998 - accuracy: 0.7182 911MiB\n",
      " 32/235 [===>..........................] - ETA: 15s - loss: 0.8909 - accuracy: 0.7206 911MiB\n",
      " 33/235 [===>..........................] - ETA: 15s - loss: 0.8818 - accuracy: 0.7228 911MiB\n",
      " 34/235 [===>..........................] - ETA: 15s - loss: 0.8728 - accuracy: 0.7254 911MiB\n",
      " 35/235 [===>..........................] - ETA: 14s - loss: 0.8684 - accuracy: 0.7275 911MiB\n",
      " 36/235 [===>..........................] - ETA: 14s - loss: 0.8552 - accuracy: 0.7321 911MiB\n",
      " 37/235 [===>..........................] - ETA: 14s - loss: 0.8451 - accuracy: 0.7356 911MiB\n",
      " 38/235 [===>..........................] - ETA: 14s - loss: 0.8330 - accuracy: 0.7395 911MiB\n",
      " 39/235 [===>..........................] - ETA: 14s - loss: 0.8202 - accuracy: 0.7441 911MiB\n",
      " 40/235 [====>.........................] - ETA: 14s - loss: 0.8112 - accuracy: 0.7473 911MiB\n",
      " 41/235 [====>.........................] - ETA: 14s - loss: 0.7997 - accuracy: 0.7512 911MiB\n",
      " 42/235 [====>.........................] - ETA: 14s - loss: 0.7895 - accuracy: 0.7544 911MiB\n",
      " 43/235 [====>.........................] - ETA: 14s - loss: 0.7789 - accuracy: 0.7583 911MiB\n",
      " 44/235 [====>.........................] - ETA: 14s - loss: 0.7712 - accuracy: 0.7609 911MiB\n",
      " 45/235 [====>.........................] - ETA: 14s - loss: 0.7614 - accuracy: 0.7639 911MiB\n",
      " 46/235 [====>.........................] - ETA: 14s - loss: 0.7560 - accuracy: 0.7660 911MiB\n",
      " 47/235 [=====>........................] - ETA: 14s - loss: 0.7474 - accuracy: 0.7687 911MiB\n",
      " 48/235 [=====>........................] - ETA: 13s - loss: 0.7399 - accuracy: 0.7711 911MiB\n",
      " 49/235 [=====>........................] - ETA: 13s - loss: 0.7352 - accuracy: 0.7722 911MiB\n",
      " 50/235 [=====>........................] - ETA: 13s - loss: 0.7334 - accuracy: 0.7732 911MiB\n",
      " 51/235 [=====>........................] - ETA: 13s - loss: 0.7271 - accuracy: 0.7750 911MiB\n",
      " 52/235 [=====>........................] - ETA: 13s - loss: 0.7214 - accuracy: 0.7767 911MiB\n",
      " 53/235 [=====>........................] - ETA: 13s - loss: 0.7137 - accuracy: 0.7793 911MiB\n",
      " 54/235 [=====>........................] - ETA: 13s - loss: 0.7081 - accuracy: 0.7812 911MiB\n",
      " 55/235 [======>.......................] - ETA: 13s - loss: 0.7043 - accuracy: 0.7822 911MiB\n",
      " 56/235 [======>.......................] - ETA: 13s - loss: 0.7004 - accuracy: 0.7830 911MiB\n",
      " 57/235 [======>.......................] - ETA: 13s - loss: 0.6968 - accuracy: 0.7841 911MiB\n",
      " 58/235 [======>.......................] - ETA: 13s - loss: 0.6976 - accuracy: 0.7832 911MiB\n",
      " 59/235 [======>.......................] - ETA: 13s - loss: 0.6927 - accuracy: 0.7845 911MiB\n",
      " 60/235 [======>.......................] - ETA: 12s - loss: 0.6886 - accuracy: 0.7859 911MiB\n",
      " 61/235 [======>.......................] - ETA: 12s - loss: 0.6829 - accuracy: 0.7878 911MiB\n",
      " 62/235 [======>.......................] - ETA: 12s - loss: 0.6777 - accuracy: 0.7895 911MiB\n",
      " 63/235 [=======>......................] - ETA: 12s - loss: 0.6756 - accuracy: 0.7903 911MiB\n",
      " 64/235 [=======>......................] - ETA: 12s - loss: 0.6694 - accuracy: 0.7921 911MiB\n",
      " 65/235 [=======>......................] - ETA: 12s - loss: 0.6640 - accuracy: 0.7942 911MiB\n",
      " 66/235 [=======>......................] - ETA: 12s - loss: 0.6600 - accuracy: 0.7953 911MiB\n",
      " 67/235 [=======>......................] - ETA: 12s - loss: 0.6554 - accuracy: 0.7966 911MiB\n",
      " 68/235 [=======>......................] - ETA: 12s - loss: 0.6498 - accuracy: 0.7984 911MiB\n",
      " 69/235 [=======>......................] - ETA: 12s - loss: 0.6456 - accuracy: 0.7996 911MiB\n",
      " 70/235 [=======>......................] - ETA: 12s - loss: 0.6427 - accuracy: 0.8003 911MiB\n",
      " 71/235 [========>.....................] - ETA: 12s - loss: 0.6391 - accuracy: 0.8013 911MiB\n",
      " 72/235 [========>.....................] - ETA: 11s - loss: 0.6338 - accuracy: 0.8031 911MiB\n",
      " 73/235 [========>.....................] - ETA: 11s - loss: 0.6291 - accuracy: 0.8050 911MiB\n",
      " 74/235 [========>.....................] - ETA: 11s - loss: 0.6240 - accuracy: 0.8069 911MiB\n",
      " 75/235 [========>.....................] - ETA: 11s - loss: 0.6192 - accuracy: 0.8086 911MiB\n",
      " 76/235 [========>.....................] - ETA: 11s - loss: 0.6151 - accuracy: 0.8098 911MiB\n",
      " 77/235 [========>.....................] - ETA: 11s - loss: 0.6118 - accuracy: 0.8108 911MiB\n",
      " 78/235 [========>.....................] - ETA: 11s - loss: 0.6070 - accuracy: 0.8123 911MiB\n",
      " 79/235 [=========>....................] - ETA: 11s - loss: 0.6046 - accuracy: 0.8130 911MiB\n",
      " 80/235 [=========>....................] - ETA: 11s - loss: 0.6009 - accuracy: 0.8143 911MiB\n",
      " 81/235 [=========>....................] - ETA: 11s - loss: 0.5983 - accuracy: 0.8152 911MiB\n",
      " 82/235 [=========>....................] - ETA: 11s - loss: 0.5970 - accuracy: 0.8158 911MiB\n",
      " 83/235 [=========>....................] - ETA: 11s - loss: 0.5931 - accuracy: 0.8170 911MiB\n",
      " 84/235 [=========>....................] - ETA: 11s - loss: 0.5893 - accuracy: 0.8181 911MiB\n",
      " 85/235 [=========>....................] - ETA: 11s - loss: 0.5862 - accuracy: 0.8191 911MiB\n",
      " 86/235 [=========>....................] - ETA: 10s - loss: 0.5814 - accuracy: 0.8205 911MiB\n",
      " 87/235 [==========>...................] - ETA: 10s - loss: 0.5787 - accuracy: 0.8212 911MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88/235 [==========>...................] - ETA: 10s - loss: 0.5756 - accuracy: 0.8222 911MiB\n",
      " 89/235 [==========>...................] - ETA: 10s - loss: 0.5746 - accuracy: 0.8225 911MiB\n",
      " 90/235 [==========>...................] - ETA: 10s - loss: 0.5712 - accuracy: 0.8233 911MiB\n",
      " 91/235 [==========>...................] - ETA: 10s - loss: 0.5676 - accuracy: 0.8244 911MiB\n",
      " 92/235 [==========>...................] - ETA: 10s - loss: 0.5635 - accuracy: 0.8255 911MiB\n",
      " 93/235 [==========>...................] - ETA: 10s - loss: 0.5607 - accuracy: 0.8264 911MiB\n",
      " 94/235 [===========>..................] - ETA: 10s - loss: 0.5587 - accuracy: 0.8272 911MiB\n",
      " 95/235 [===========>..................] - ETA: 10s - loss: 0.5561 - accuracy: 0.8281 911MiB\n",
      " 96/235 [===========>..................] - ETA: 10s - loss: 0.5531 - accuracy: 0.8289 911MiB\n",
      " 97/235 [===========>..................] - ETA: 10s - loss: 0.5514 - accuracy: 0.8294 911MiB\n",
      " 98/235 [===========>..................] - ETA: 10s - loss: 0.5487 - accuracy: 0.8302 911MiB\n",
      " 99/235 [===========>..................] - ETA: 10s - loss: 0.5464 - accuracy: 0.8310 911MiB\n",
      "100/235 [===========>..................] - ETA: 9s - loss: 0.5430 - accuracy: 0.8321  911MiB\n",
      "101/235 [===========>..................] - ETA: 9s - loss: 0.5401 - accuracy: 0.8330 911MiB\n",
      "102/235 [============>.................] - ETA: 9s - loss: 0.5365 - accuracy: 0.8341 911MiB\n",
      "103/235 [============>.................] - ETA: 9s - loss: 0.5334 - accuracy: 0.8352 911MiB\n",
      "104/235 [============>.................] - ETA: 9s - loss: 0.5315 - accuracy: 0.8359 911MiB\n",
      "105/235 [============>.................] - ETA: 9s - loss: 0.5312 - accuracy: 0.8359 911MiB\n",
      "106/235 [============>.................] - ETA: 9s - loss: 0.5288 - accuracy: 0.8367 911MiB\n",
      "107/235 [============>.................] - ETA: 9s - loss: 0.5269 - accuracy: 0.8375 911MiB\n",
      "108/235 [============>.................] - ETA: 9s - loss: 0.5246 - accuracy: 0.8381 911MiB\n",
      "109/235 [============>.................] - ETA: 9s - loss: 0.5224 - accuracy: 0.8389 911MiB\n",
      "110/235 [=============>................] - ETA: 9s - loss: 0.5194 - accuracy: 0.8399 911MiB\n",
      "111/235 [=============>................] - ETA: 9s - loss: 0.5172 - accuracy: 0.8407 911MiB\n",
      "112/235 [=============>................] - ETA: 9s - loss: 0.5151 - accuracy: 0.8414 911MiB\n",
      "113/235 [=============>................] - ETA: 8s - loss: 0.5121 - accuracy: 0.8424 911MiB\n",
      "114/235 [=============>................] - ETA: 8s - loss: 0.5100 - accuracy: 0.8429 911MiB\n",
      "115/235 [=============>................] - ETA: 8s - loss: 0.5081 - accuracy: 0.8434 911MiB\n",
      "116/235 [=============>................] - ETA: 8s - loss: 0.5055 - accuracy: 0.8442 911MiB\n",
      "117/235 [=============>................] - ETA: 8s - loss: 0.5043 - accuracy: 0.8447 911MiB\n",
      "118/235 [==============>...............] - ETA: 8s - loss: 0.5036 - accuracy: 0.8449 911MiB\n",
      "119/235 [==============>...............] - ETA: 8s - loss: 0.5015 - accuracy: 0.8456 911MiB\n",
      "120/235 [==============>...............] - ETA: 8s - loss: 0.4998 - accuracy: 0.8463 911MiB\n",
      "121/235 [==============>...............] - ETA: 8s - loss: 0.4978 - accuracy: 0.8467 911MiB\n",
      "122/235 [==============>...............] - ETA: 8s - loss: 0.4962 - accuracy: 0.8473 911MiB\n",
      "123/235 [==============>...............] - ETA: 8s - loss: 0.4952 - accuracy: 0.8478 911MiB\n",
      "124/235 [==============>...............] - ETA: 8s - loss: 0.4940 - accuracy: 0.8481 911MiB\n",
      "125/235 [==============>...............] - ETA: 8s - loss: 0.4923 - accuracy: 0.8486 911MiB\n",
      "126/235 [===============>..............] - ETA: 7s - loss: 0.4909 - accuracy: 0.8489 911MiB\n",
      "127/235 [===============>..............] - ETA: 7s - loss: 0.4898 - accuracy: 0.8492 911MiB\n",
      "128/235 [===============>..............] - ETA: 7s - loss: 0.4881 - accuracy: 0.8499 911MiB\n",
      "129/235 [===============>..............] - ETA: 7s - loss: 0.4862 - accuracy: 0.8506 911MiB\n",
      "130/235 [===============>..............] - ETA: 7s - loss: 0.4838 - accuracy: 0.8513 911MiB\n",
      "131/235 [===============>..............] - ETA: 7s - loss: 0.4821 - accuracy: 0.8520 911MiB\n",
      "132/235 [===============>..............] - ETA: 7s - loss: 0.4801 - accuracy: 0.8526 911MiB\n",
      "133/235 [===============>..............] - ETA: 7s - loss: 0.4773 - accuracy: 0.8535 911MiB\n",
      "134/235 [================>.............] - ETA: 7s - loss: 0.4750 - accuracy: 0.8542 911MiB\n",
      "135/235 [================>.............] - ETA: 7s - loss: 0.4736 - accuracy: 0.8545 911MiB\n",
      "136/235 [================>.............] - ETA: 7s - loss: 0.4723 - accuracy: 0.8549 911MiB\n",
      "137/235 [================>.............] - ETA: 7s - loss: 0.4707 - accuracy: 0.8554 911MiB\n",
      "138/235 [================>.............] - ETA: 7s - loss: 0.4689 - accuracy: 0.8561 911MiB\n",
      "139/235 [================>.............] - ETA: 6s - loss: 0.4672 - accuracy: 0.8566 911MiB\n",
      "140/235 [================>.............] - ETA: 6s - loss: 0.4652 - accuracy: 0.8573 911MiB\n",
      "141/235 [=================>............] - ETA: 6s - loss: 0.4636 - accuracy: 0.8577 911MiB\n",
      "142/235 [=================>............] - ETA: 6s - loss: 0.4621 - accuracy: 0.8582 911MiB\n",
      "143/235 [=================>............] - ETA: 6s - loss: 0.4607 - accuracy: 0.8586 911MiB\n",
      "144/235 [=================>............] - ETA: 6s - loss: 0.4590 - accuracy: 0.8592 911MiB\n",
      "145/235 [=================>............] - ETA: 6s - loss: 0.4578 - accuracy: 0.8596 911MiB\n",
      "146/235 [=================>............] - ETA: 6s - loss: 0.4568 - accuracy: 0.8599 911MiB\n",
      "147/235 [=================>............] - ETA: 6s - loss: 0.4561 - accuracy: 0.8602 911MiB\n",
      "148/235 [=================>............] - ETA: 6s - loss: 0.4549 - accuracy: 0.8606 911MiB\n",
      "149/235 [==================>...........] - ETA: 6s - loss: 0.4531 - accuracy: 0.8611 911MiB\n",
      "150/235 [==================>...........] - ETA: 6s - loss: 0.4515 - accuracy: 0.8616 911MiB\n",
      "151/235 [==================>...........] - ETA: 6s - loss: 0.4500 - accuracy: 0.8621 911MiB\n",
      "152/235 [==================>...........] - ETA: 5s - loss: 0.4480 - accuracy: 0.8628 911MiB\n",
      "153/235 [==================>...........] - ETA: 5s - loss: 0.4460 - accuracy: 0.8634 911MiB\n",
      "154/235 [==================>...........] - ETA: 5s - loss: 0.4450 - accuracy: 0.8638 911MiB\n",
      "155/235 [==================>...........] - ETA: 5s - loss: 0.4437 - accuracy: 0.8642 911MiB\n",
      "156/235 [==================>...........] - ETA: 5s - loss: 0.4425 - accuracy: 0.8645 911MiB\n",
      "157/235 [===================>..........] - ETA: 5s - loss: 0.4412 - accuracy: 0.8648 911MiB\n",
      "158/235 [===================>..........] - ETA: 5s - loss: 0.4401 - accuracy: 0.8650 911MiB\n",
      "159/235 [===================>..........] - ETA: 5s - loss: 0.4389 - accuracy: 0.8655 911MiB\n",
      "160/235 [===================>..........] - ETA: 5s - loss: 0.4371 - accuracy: 0.8661 911MiB\n",
      "161/235 [===================>..........] - ETA: 5s - loss: 0.4358 - accuracy: 0.8665 911MiB\n",
      "162/235 [===================>..........] - ETA: 5s - loss: 0.4350 - accuracy: 0.8667 911MiB\n",
      "163/235 [===================>..........] - ETA: 5s - loss: 0.4341 - accuracy: 0.8670 911MiB\n",
      "164/235 [===================>..........] - ETA: 5s - loss: 0.4329 - accuracy: 0.8674 911MiB\n",
      "165/235 [====================>.........] - ETA: 5s - loss: 0.4318 - accuracy: 0.8678 911MiB\n",
      "166/235 [====================>.........] - ETA: 4s - loss: 0.4309 - accuracy: 0.8681 911MiB\n",
      "167/235 [====================>.........] - ETA: 4s - loss: 0.4296 - accuracy: 0.8684 911MiB\n",
      "168/235 [====================>.........] - ETA: 4s - loss: 0.4290 - accuracy: 0.8687 911MiB\n",
      "169/235 [====================>.........] - ETA: 4s - loss: 0.4277 - accuracy: 0.8690 911MiB\n",
      "170/235 [====================>.........] - ETA: 4s - loss: 0.4258 - accuracy: 0.8696 911MiB\n",
      "171/235 [====================>.........] - ETA: 4s - loss: 0.4244 - accuracy: 0.8701 911MiB\n",
      "172/235 [====================>.........] - ETA: 4s - loss: 0.4232 - accuracy: 0.8704 911MiB\n",
      "173/235 [=====================>........] - ETA: 4s - loss: 0.4218 - accuracy: 0.8707 911MiB\n",
      "174/235 [=====================>........] - ETA: 4s - loss: 0.4209 - accuracy: 0.8710 911MiB\n",
      "175/235 [=====================>........] - ETA: 4s - loss: 0.4193 - accuracy: 0.8715 911MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/235 [=====================>........] - ETA: 4s - loss: 0.4186 - accuracy: 0.8717 911MiB\n",
      "177/235 [=====================>........] - ETA: 4s - loss: 0.4179 - accuracy: 0.8720 911MiB\n",
      "178/235 [=====================>........] - ETA: 4s - loss: 0.4166 - accuracy: 0.8723 911MiB\n",
      "179/235 [=====================>........] - ETA: 4s - loss: 0.4156 - accuracy: 0.8727 911MiB\n",
      "180/235 [=====================>........] - ETA: 3s - loss: 0.4147 - accuracy: 0.8729 911MiB\n",
      "181/235 [======================>.......] - ETA: 3s - loss: 0.4143 - accuracy: 0.8731 911MiB\n",
      "182/235 [======================>.......] - ETA: 3s - loss: 0.4132 - accuracy: 0.8734 911MiB\n",
      "183/235 [======================>.......] - ETA: 3s - loss: 0.4119 - accuracy: 0.8738 911MiB\n",
      "184/235 [======================>.......] - ETA: 3s - loss: 0.4105 - accuracy: 0.8742 911MiB\n",
      "185/235 [======================>.......] - ETA: 3s - loss: 0.4098 - accuracy: 0.8745 911MiB\n",
      "186/235 [======================>.......] - ETA: 3s - loss: 0.4088 - accuracy: 0.8747 911MiB\n",
      "187/235 [======================>.......] - ETA: 3s - loss: 0.4074 - accuracy: 0.8752 911MiB\n",
      "188/235 [=======================>......] - ETA: 3s - loss: 0.4064 - accuracy: 0.8756 911MiB\n",
      "189/235 [=======================>......] - ETA: 3s - loss: 0.4052 - accuracy: 0.8759 911MiB\n",
      "190/235 [=======================>......] - ETA: 3s - loss: 0.4040 - accuracy: 0.8763 911MiB\n",
      "191/235 [=======================>......] - ETA: 3s - loss: 0.4027 - accuracy: 0.8767 911MiB\n",
      "192/235 [=======================>......] - ETA: 3s - loss: 0.4026 - accuracy: 0.8768 911MiB\n",
      "193/235 [=======================>......] - ETA: 2s - loss: 0.4017 - accuracy: 0.8770 911MiB\n",
      "194/235 [=======================>......] - ETA: 2s - loss: 0.4014 - accuracy: 0.8771 911MiB\n",
      "195/235 [=======================>......] - ETA: 2s - loss: 0.4008 - accuracy: 0.8774 911MiB\n",
      "196/235 [========================>.....] - ETA: 2s - loss: 0.3996 - accuracy: 0.8777 911MiB\n",
      "197/235 [========================>.....] - ETA: 2s - loss: 0.3995 - accuracy: 0.8779 911MiB\n",
      "198/235 [========================>.....] - ETA: 2s - loss: 0.3985 - accuracy: 0.8782 911MiB\n",
      "199/235 [========================>.....] - ETA: 2s - loss: 0.3972 - accuracy: 0.8786 911MiB\n",
      "200/235 [========================>.....] - ETA: 2s - loss: 0.3957 - accuracy: 0.8791 911MiB\n",
      "201/235 [========================>.....] - ETA: 2s - loss: 0.3947 - accuracy: 0.8794 911MiB\n",
      "202/235 [========================>.....] - ETA: 2s - loss: 0.3936 - accuracy: 0.8798 911MiB\n",
      "203/235 [========================>.....] - ETA: 2s - loss: 0.3924 - accuracy: 0.8802 911MiB\n",
      "204/235 [=========================>....] - ETA: 2s - loss: 0.3918 - accuracy: 0.8804 911MiB\n",
      "205/235 [=========================>....] - ETA: 2s - loss: 0.3907 - accuracy: 0.8807 911MiB\n",
      "206/235 [=========================>....] - ETA: 2s - loss: 0.3896 - accuracy: 0.8810 911MiB\n",
      "207/235 [=========================>....] - ETA: 1s - loss: 0.3893 - accuracy: 0.8811 911MiB\n",
      "208/235 [=========================>....] - ETA: 1s - loss: 0.3886 - accuracy: 0.8813 911MiB\n",
      "209/235 [=========================>....] - ETA: 1s - loss: 0.3872 - accuracy: 0.8817 911MiB\n",
      "210/235 [=========================>....] - ETA: 1s - loss: 0.3864 - accuracy: 0.8819 911MiB\n",
      "211/235 [=========================>....] - ETA: 1s - loss: 0.3859 - accuracy: 0.8821 911MiB\n",
      "212/235 [==========================>...] - ETA: 1s - loss: 0.3848 - accuracy: 0.8824 911MiB\n",
      "213/235 [==========================>...] - ETA: 1s - loss: 0.3836 - accuracy: 0.8827 911MiB\n",
      "214/235 [==========================>...] - ETA: 1s - loss: 0.3824 - accuracy: 0.8831 911MiB\n",
      "215/235 [==========================>...] - ETA: 1s - loss: 0.3815 - accuracy: 0.8833 911MiB\n",
      "216/235 [==========================>...] - ETA: 1s - loss: 0.3806 - accuracy: 0.8835 911MiB\n",
      "217/235 [==========================>...] - ETA: 1s - loss: 0.3795 - accuracy: 0.8839 911MiB\n",
      "218/235 [==========================>...] - ETA: 1s - loss: 0.3785 - accuracy: 0.8842 911MiB\n",
      "219/235 [==========================>...] - ETA: 1s - loss: 0.3775 - accuracy: 0.8846 911MiB\n",
      "220/235 [===========================>..] - ETA: 1s - loss: 0.3766 - accuracy: 0.8848 911MiB\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.3755 - accuracy: 0.8851 911MiB\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.3744 - accuracy: 0.8855 911MiB\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.3732 - accuracy: 0.8858 911MiB\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.3721 - accuracy: 0.8861 911MiB\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.3710 - accuracy: 0.8864 911MiB\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.3703 - accuracy: 0.8866 911MiB\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.3692 - accuracy: 0.8869 911MiB\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8873 911MiB\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.8877 911MiB\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8880 911MiB\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.3642 - accuracy: 0.8885 911MiB\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.8888 911MiB\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.3620 - accuracy: 0.8891 911MiB\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.3614 - accuracy: 0.8894 911MiB\n",
      "235/235 [==============================] - 17s 73ms/step - loss: 0.3611 - accuracy: 0.8895 - val_loss: 0.2593 - val_accuracy: 0.9251\n",
      "Epoch 2/10\n",
      " 911MiB\n",
      "  1/235 [..............................] - ETA: 15s - loss: 0.3194 - accuracy: 0.9219 911MiB\n",
      "  2/235 [..............................] - ETA: 16s - loss: 0.2397 - accuracy: 0.9395 911MiB\n",
      "  3/235 [..............................] - ETA: 16s - loss: 0.2302 - accuracy: 0.9440 911MiB\n",
      "  4/235 [..............................] - ETA: 16s - loss: 0.2286 - accuracy: 0.9424 911MiB\n",
      "  5/235 [..............................] - ETA: 16s - loss: 0.2466 - accuracy: 0.9344 911MiB\n",
      "  6/235 [..............................] - ETA: 16s - loss: 0.2362 - accuracy: 0.9382 911MiB\n",
      "  7/235 [..............................] - ETA: 16s - loss: 0.2236 - accuracy: 0.9414 911MiB\n",
      "  8/235 [>.............................] - ETA: 16s - loss: 0.2119 - accuracy: 0.9438 911MiB\n",
      "  9/235 [>.............................] - ETA: 16s - loss: 0.2028 - accuracy: 0.9462 911MiB\n",
      " 10/235 [>.............................] - ETA: 16s - loss: 0.1934 - accuracy: 0.9488 911MiB\n",
      " 11/235 [>.............................] - ETA: 15s - loss: 0.1922 - accuracy: 0.9492 911MiB\n",
      " 12/235 [>.............................] - ETA: 15s - loss: 0.1879 - accuracy: 0.9499 911MiB\n",
      " 13/235 [>.............................] - ETA: 15s - loss: 0.1849 - accuracy: 0.9495 911MiB\n",
      " 14/235 [>.............................] - ETA: 15s - loss: 0.1800 - accuracy: 0.9503 911MiB\n",
      " 15/235 [>.............................] - ETA: 15s - loss: 0.1797 - accuracy: 0.9490 911MiB\n",
      " 16/235 [=>............................] - ETA: 15s - loss: 0.1746 - accuracy: 0.9507 911MiB\n",
      " 17/235 [=>............................] - ETA: 15s - loss: 0.1748 - accuracy: 0.9506 911MiB\n",
      " 18/235 [=>............................] - ETA: 15s - loss: 0.1716 - accuracy: 0.9518 911MiB\n",
      " 19/235 [=>............................] - ETA: 15s - loss: 0.1683 - accuracy: 0.9525 911MiB\n",
      " 20/235 [=>............................] - ETA: 15s - loss: 0.1673 - accuracy: 0.9531 911MiB\n",
      " 21/235 [=>............................] - ETA: 15s - loss: 0.1663 - accuracy: 0.9535 911MiB\n",
      " 22/235 [=>............................] - ETA: 15s - loss: 0.1639 - accuracy: 0.9540 911MiB\n",
      " 23/235 [=>............................] - ETA: 15s - loss: 0.1657 - accuracy: 0.9530 911MiB\n",
      " 24/235 [==>...........................] - ETA: 15s - loss: 0.1657 - accuracy: 0.9536 911MiB\n",
      " 25/235 [==>...........................] - ETA: 15s - loss: 0.1647 - accuracy: 0.9538 911MiB\n",
      " 26/235 [==>...........................] - ETA: 15s - loss: 0.1631 - accuracy: 0.9543 911MiB\n",
      " 27/235 [==>...........................] - ETA: 15s - loss: 0.1646 - accuracy: 0.9537 911MiB\n",
      " 28/235 [==>...........................] - ETA: 15s - loss: 0.1653 - accuracy: 0.9530 911MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29/235 [==>...........................] - ETA: 15s - loss: 0.1680 - accuracy: 0.9515 911MiB\n",
      " 30/235 [==>...........................] - ETA: 14s - loss: 0.1693 - accuracy: 0.9509 911MiB\n",
      " 31/235 [==>...........................] - ETA: 14s - loss: 0.1700 - accuracy: 0.9507 911MiB\n",
      " 32/235 [===>..........................] - ETA: 14s - loss: 0.1704 - accuracy: 0.9501 911MiB\n",
      " 33/235 [===>..........................] - ETA: 14s - loss: 0.1707 - accuracy: 0.9500 911MiB\n",
      " 34/235 [===>..........................] - ETA: 14s - loss: 0.1711 - accuracy: 0.9498 911MiB\n",
      " 35/235 [===>..........................] - ETA: 14s - loss: 0.1767 - accuracy: 0.9482 911MiB\n",
      " 36/235 [===>..........................] - ETA: 14s - loss: 0.1751 - accuracy: 0.9487 911MiB\n",
      " 37/235 [===>..........................] - ETA: 14s - loss: 0.1764 - accuracy: 0.9481 911MiB\n",
      " 38/235 [===>..........................] - ETA: 14s - loss: 0.1754 - accuracy: 0.9482 911MiB\n",
      " 39/235 [===>..........................] - ETA: 14s - loss: 0.1734 - accuracy: 0.9487 911MiB\n",
      " 40/235 [====>.........................] - ETA: 14s - loss: 0.1745 - accuracy: 0.9483 911MiB\n",
      " 41/235 [====>.........................] - ETA: 14s - loss: 0.1723 - accuracy: 0.9491 911MiB\n",
      " 42/235 [====>.........................] - ETA: 14s - loss: 0.1707 - accuracy: 0.9498 911MiB\n",
      " 43/235 [====>.........................] - ETA: 13s - loss: 0.1695 - accuracy: 0.9499 911MiB\n",
      " 44/235 [====>.........................] - ETA: 13s - loss: 0.1683 - accuracy: 0.9505 911MiB\n",
      " 45/235 [====>.........................] - ETA: 13s - loss: 0.1663 - accuracy: 0.9510 911MiB\n",
      " 46/235 [====>.........................] - ETA: 13s - loss: 0.1671 - accuracy: 0.9508 911MiB\n",
      " 47/235 [=====>........................] - ETA: 13s - loss: 0.1666 - accuracy: 0.9508 911MiB\n",
      " 48/235 [=====>........................] - ETA: 13s - loss: 0.1668 - accuracy: 0.9508 911MiB\n",
      " 49/235 [=====>........................] - ETA: 13s - loss: 0.1674 - accuracy: 0.9506 911MiB\n",
      " 50/235 [=====>........................] - ETA: 13s - loss: 0.1694 - accuracy: 0.9502 911MiB\n",
      " 51/235 [=====>........................] - ETA: 13s - loss: 0.1696 - accuracy: 0.9501 911MiB\n",
      " 52/235 [=====>........................] - ETA: 13s - loss: 0.1694 - accuracy: 0.9503 911MiB\n",
      " 53/235 [=====>........................] - ETA: 13s - loss: 0.1684 - accuracy: 0.9508 911MiB\n",
      " 54/235 [=====>........................] - ETA: 13s - loss: 0.1677 - accuracy: 0.9507 911MiB\n",
      " 55/235 [======>.......................] - ETA: 12s - loss: 0.1679 - accuracy: 0.9508 911MiB\n",
      " 56/235 [======>.......................] - ETA: 12s - loss: 0.1677 - accuracy: 0.9510 911MiB\n",
      " 57/235 [======>.......................] - ETA: 12s - loss: 0.1669 - accuracy: 0.9509 911MiB\n",
      " 58/235 [======>.......................] - ETA: 12s - loss: 0.1678 - accuracy: 0.9509"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cb3126323d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     epochs = 10, callbacks = [mem_check])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n\u001b[1;32m    271\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unscaled_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         logging.warning('The list of trainable weights is empty. Make sure that'\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    439\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m           kwargs={\"name\": name})\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   1923\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    483\u001b[0m           update_ops.extend(\n\u001b[1;32m    484\u001b[0m               distribution.extended.update(\n\u001b[0;32m--> 485\u001b[0;31m                   var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m       any_symbolic = any(isinstance(i, ops.Operation) or\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   1528\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2140\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mread_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_local_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m                           expand_composites=expand_composites)\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    529\u001b[0m                           expand_composites=expand_composites)\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m   \"\"\"\n\u001b[0;32m--> 262\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mFlatten\u001b[0;34m(nested, expand_composites)\u001b[0m\n\u001b[1;32m   2617\u001b[0m \u001b[0mAssertSameStructure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAssertSameStructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2619\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2620\u001b[0m     \"\"\"\n\u001b[1;32m   2621\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mflat\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mnested\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = ds_train, \n",
    "                    steps_per_epoch = int(np.ceil(num_samples /float(BATCH_SIZE))), \n",
    "                    validation_data = ds_test, \n",
    "                    validation_steps = int(np.ceil(x_test.shape[0] /float(BATCH_SIZE))),\n",
    "                    \n",
    "                    epochs = 10, callbacks = [mem_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
