{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "bi-daf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uECguKR3fCjE",
        "colab_type": "text"
      },
      "source": [
        "Credit : https://github.com/ParikhKadam/bidaf-keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xg1d_uifj5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "9333128d-d26f-43ce-82ac-32de3d978f73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "!mkdir cudnn\n",
        "!tar -xzvf /gdrive/My\\ Drive/cuDNN/cudnn-10.0-linux-x64-v7.6.4.38.tgz -C cudnn\n",
        "\n",
        "!cp cudnn/cuda/include/cudnn.h /usr/local/cuda/include\n",
        "!cp cudnn/cuda/lib64/libcudnn* /usr/local/cuda/lib64\n",
        "!chmod a+r /usr/local/cuda/lib64/libcudnn*\n",
        "# Extracts the cuDNN files from Drive folder directly to the VM CUDA folders\n",
        "!chmod a+r /usr/local/cuda/include/cudnn.h\n",
        "\n",
        "# Now we check the version we already installed. Can comment this line on future runs\n",
        "!cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n",
        "\n",
        "!git clone -l -s https://github.com/anirudhshenoy/bidaf-tf_2.0-aic.git\n",
        "!mv /content/bidaf-tf_2.0-aic/* /content\n",
        "!mkdir data/magnitude\n",
        "!cp /gdrive/'My Drive'/glove.6B.100d.magnitude  data/magnitude\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "!pip3 install pymagnitude\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls7ydGq9fCjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5lv3tVOmghI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y14AgsWvfCjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQVBtgBcfCjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Credit to : https://www.kaggle.com/sanjay11100/squad-stanford-q-a-json-to-pandas-dataframe\n",
        "# Modified to include first answer_start and text for dev set\n",
        "\n",
        "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
        "                           verbose = 1):\n",
        "    \"\"\"\n",
        "    input_file_path: path to the squad json file.\n",
        "    record_path: path to deepest level in json file default value is\n",
        "    ['data','paragraphs','qas','answers']\n",
        "    verbose: 0 to suppress it default is 1\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(input_file_path).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "    m['context'] = idx\n",
        "    js['q_idx'] = ndx\n",
        "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main\n",
        "\n",
        "\n",
        "def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
        "                           verbose = 1):\n",
        "    \"\"\"\n",
        "    input_file_path: path to the squad json file.\n",
        "    record_path: path to deepest level in json file default value is\n",
        "    ['data','paragraphs','qas','answers']\n",
        "    verbose: 0 to suppress it default is 1\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(input_file_path).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "    m['context'] = idx\n",
        "#     js['q_idx'] = ndx\n",
        "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    answer_start = []\n",
        "    answer_text = []\n",
        "\n",
        "    for answers in tqdm_notebook(main['answers'].values):\n",
        "        answer_start.append(answers[0]['answer_start'])\n",
        "        answer_text.append(answers[0]['text'])\n",
        "\n",
        "    main['answer_start'] = answer_start\n",
        "    main['text'] = answer_text\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6piYR8irfCjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = squad_json_to_dataframe_train('./data/train-v1.1.json')\n",
        "df_dev = squad_json_to_dataframe_dev('./data/dev-v1.1.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i26V4cv1fCjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_NUM_SAMPLES = df_train.shape[0]\n",
        "DEV_NUM_SAMPLES = df_dev.shape[0]\n",
        "\n",
        "\n",
        "#df_train = df_train[:TRAIN_NUM_SAMPLES]\n",
        "#df_dev = df_dev[:DEV_NUM_SAMPLES]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lst6aOhufCjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "from nltk import word_tokenize\n",
        "\n",
        "answer_start = []\n",
        "answer_end = []\n",
        "for i in tqdm_notebook(range(df_train.shape[0])): \n",
        "    context_split = word_tokenize(df_train.context.values[i][:df_train.answer_start.values[i]])\n",
        "    answer_start.append(len(context_split))\n",
        "    answer_end.append(len(context_split) + len(word_tokenize(df_train.text.values[i])) -1)\n",
        "df_train['answer_end'] = answer_end\n",
        "df_train['answer_start'] = answer_start\n",
        "\n",
        "answer_start = []\n",
        "answer_end = []\n",
        "for i in tqdm_notebook(range(df_dev.shape[0])): \n",
        "    context_split = word_tokenize(df_dev.context.values[i][:df_dev.answer_start.values[i]])\n",
        "    answer_start.append(len(context_split))\n",
        "    answer_end.append(len(context_split) + len(word_tokenize(df_dev.text.values[i])) -1)\n",
        "df_dev['answer_end'] = answer_end\n",
        "df_dev['answer_start'] = answer_start\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVxBYEL4fCjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymagnitude import MagnitudeUtils, Magnitude\n",
        "#from scripts import MagnitudeVectors\n",
        "\n",
        "#vectors = MagnitudeVectors(50).load_vectors()\n",
        "vectors = Magnitude('./data/magnitude/glove.6B.100d.magnitude')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfK8sHBYfCjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.question = [word_tokenize(q) for q in tqdm_notebook(df_train.question.values)]\n",
        "df_train.context = [word_tokenize(q) for q in tqdm_notebook(df_train.context.values)]\n",
        "\n",
        "df_dev.question = [word_tokenize(q) for q in tqdm_notebook(df_dev.question.values)]\n",
        "df_dev.context = [word_tokenize(q) for q in tqdm_notebook(df_dev.context.values)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldRmpThufCjs",
        "colab_type": "raw"
      },
      "source": [
        "df_train.answer_start = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_train.answer_start.values)]\n",
        "df_train.answer_end = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_train.answer_end.values)]\n",
        "\n",
        "df_dev.answer_start = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_dev.answer_start.values)]\n",
        "df_dev.answer_end = [MagnitudeUtils.to_categorical(a, MAX_CONTEXT_LENGTH) for a in tqdm_notebook(df_dev.answer_end.values)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx-ZM0y-fCjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = df_train.answer_start.values, df_train.answer_end.values\n",
        "x_train = df_train.context.values, df_train.question.values\n",
        "\n",
        "y_dev = df_dev.answer_start.values, df_dev.answer_end.values\n",
        "x_dev = df_dev.context.values, df_dev.question.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1XZpqA9fCjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "num_batches_per_epoch_train = int(np.ceil(TRAIN_NUM_SAMPLES/float(BATCH_SIZE)))\n",
        "num_batches_per_epoch_dev = int(np.ceil(DEV_NUM_SAMPLES/float(BATCH_SIZE)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EENe4dHfCjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_gen():\n",
        "    for i in range(TRAIN_NUM_SAMPLES):\n",
        "        if not (i%BATCH_SIZE):\n",
        "            context_pad_length = max([len(t) for t in x_train[0][i:i+BATCH_SIZE]])\n",
        "            question_pad_length = max([len(t) for t in x_train[1][i:i+BATCH_SIZE]])\n",
        "\n",
        "        X_context_batch = vectors.query(x_train[0][i], pad_to_length = context_pad_length)\n",
        "        X_question_batch = vectors.query(x_train[1][i], pad_to_length = question_pad_length)\n",
        "\n",
        "        Y_start_batch = tf.keras.utils.to_categorical(y_train[0][i],context_pad_length)\n",
        "        Y_end_batch = tf.keras.utils.to_categorical(y_train[1][i],context_pad_length)\n",
        "        \n",
        "        yield ((tf.constant(X_context_batch), tf.constant(X_question_batch)), (tf.constant(Y_start_batch), tf.constant(Y_end_batch)))\n",
        "        \n",
        "def dev_gen():\n",
        "    for i in range(DEV_NUM_SAMPLES):\n",
        "        if not (i%BATCH_SIZE):\n",
        "            context_pad_length = max([len(t) for t in x_dev[0][i:i+BATCH_SIZE]])\n",
        "            question_pad_length = max([len(t) for t in x_dev[1][i:i+BATCH_SIZE]])\n",
        "        X_context_batch = vectors.query(x_dev[0][i], pad_to_length = context_pad_length)\n",
        "        X_question_batch = vectors.query(x_dev[1][i], pad_to_length = question_pad_length)\n",
        "\n",
        "        Y_start_batch = tf.keras.utils.to_categorical(y_dev[0][i],context_pad_length)\n",
        "        Y_end_batch =tf.keras.utils.to_categorical(y_dev[1][i],context_pad_length)\n",
        "        \n",
        "        yield ((tf.constant(X_context_batch), tf.constant(X_question_batch)), (tf.constant(Y_start_batch), tf.constant(Y_end_batch)))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uWH01ZUfCj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train = tf.data.Dataset.from_generator(train_gen, ((tf.float32, tf.float32), (tf.float32, tf.float32))).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "dataset_dev = tf.data.Dataset.from_generator(dev_gen, ((tf.float32, tf.float32), (tf.float32, tf.float32))).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f79M1uoyfCkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMyYQLZj0cO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SaveWeights(tf.keras.callbacks.Callback):\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        if not (batch%100):\n",
        "            print('Saving Temp Weights')\n",
        "            self.model.save_weights('/gdrive/My\\ Drive/bi-daf/temp_weights/weights-batch-{}.h5'.format(batch))\n",
        "\n",
        "\n",
        "save_weights = SaveWeights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsfr8TYZfCkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_for_end_prob(inputs):\n",
        "    encoded_context, merged_context, modeled_context, span_begin_probabilities = inputs\n",
        "    weighted_sum = K.sum(K.expand_dims(span_begin_probabilities, axis=-1) * modeled_context, -2)\n",
        "    passage_weighted_by_predicted_span = K.expand_dims(weighted_sum, axis=1)\n",
        "    tile_shape = K.concatenate([[1], [K.shape(encoded_context)[1]], [1]], axis=0)\n",
        "    passage_weighted_by_predicted_span = K.tile(passage_weighted_by_predicted_span, tile_shape)\n",
        "    multiply1 = modeled_context * passage_weighted_by_predicted_span\n",
        "    span_end_representation = K.concatenate(\n",
        "            [merged_context, modeled_context, passage_weighted_by_predicted_span, multiply1])\n",
        "\n",
        "    return span_end_representation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ivhM3HfCkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBED_LENGTH = 100\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Concatenate, TimeDistributed, Dense, Softmax, Flatten, Lambda, Multiply, Add, Dropout\n",
        "from tensorflow.keras.models import Model \n",
        "from tensorflow.keras.optimizers import Adam, Adadelta\n",
        "from tensorflow.keras.activations import linear\n",
        "from layers import Similarity, C2QAttention, Q2CAttention, MergedContext, SpanBegin, SpanEnd, Highway\n",
        "\n",
        "######## INPUT LAYER #########\n",
        "context_input = Input(shape = (None, EMBED_LENGTH), dtype = 'float32', name = 'context_input')\n",
        "question_input = Input(shape = (None, EMBED_LENGTH), dtype = 'float32', name = 'question_input')\n",
        "\n",
        "\n",
        "####### HIGHWAY LAYER #########\n",
        "\n",
        "highway_layer = Highway(name='highway_1')\n",
        "                        \n",
        "question_layer = TimeDistributed(highway_layer, name='highway_qtd')\n",
        "question_embedding = question_layer(question_input)\n",
        "                        \n",
        "passage_layer = TimeDistributed(highway_layer, name='highway__ptd')\n",
        "context_embedding = passage_layer(context_input)\n",
        "\n",
        "\n",
        "######## CONTEXTUAL EMBEDDING LAYER ########\n",
        "encoder_layer = Bidirectional(LSTM(64, return_sequences=True), name='bidirectional_encoder')\n",
        "encoded_question = encoder_layer(question_embedding)\n",
        "encoded_context = encoder_layer(context_embedding)\n",
        "\n",
        "\n",
        "######## SIMILARITY LAYER ########\n",
        "similarity_matrix = Similarity(name='similarity_layer')([encoded_context, encoded_question])\n",
        "\n",
        "####### ATTENTION LAYER #########\n",
        "context_to_query_attention = C2QAttention(name='context_to_query_attention')([\n",
        "            similarity_matrix, encoded_question])\n",
        "query_to_context_attention = Q2CAttention(name='query_to_context_attention')([\n",
        "            similarity_matrix, encoded_context])\n",
        "\n",
        "###### MERGE ATTENTIONS ########\n",
        "merged_context = MergedContext(name='merged_context')(\n",
        "            [encoded_context, context_to_query_attention, query_to_context_attention])\n",
        "\n",
        "###### MODELLING LAYER #########\n",
        "modeled_context = Bidirectional(LSTM(64,return_sequences=True), name='decoder')(merged_context)\n",
        "\n",
        "###### OUTPUT LAYER SPAN BEGIN#########\n",
        "span_begin_concat = K.concatenate([merged_context, modeled_context], axis = -1)\n",
        "span_begin_weights = TimeDistributed(Dense(1), name = 'Dense_span_begin')(span_begin_concat)\n",
        "#span_begin_weights = Dropout(0.2)(span_begin_weights)\n",
        "span_begin_probabilities = Softmax(name = 'span-begin-output')(K.squeeze(span_begin_weights, axis=-1))\n",
        "\n",
        "span_end_representation = Lambda(prepare_for_end_prob)((encoded_context, merged_context, modeled_context, span_begin_probabilities))\n",
        "span_end_representation = Bidirectional(LSTM(64, return_sequences=True), name='output_end_prob_decoder')(span_end_representation)\n",
        "\n",
        "span_end_input = K.concatenate([merged_context, span_end_representation])\n",
        "span_end_weights = TimeDistributed(Dense(1), name = 'Dense_span_end')(span_end_input)\n",
        "#span_end_weights = Dropout(0.2)(span_end_weights)\n",
        "span_end_probabilities = Softmax(name = 'span-end-output')(K.squeeze(span_end_weights, axis=-1))\n",
        "\n",
        "\n",
        "\n",
        "model = Model([context_input, question_input], [span_begin_probabilities, span_end_probabilities])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jEXmiN5fCkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZmQUHbufCkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam, Adadelta\n",
        "\n",
        "model.compile(optimizer = Adadelta(0.5), loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0nqaHDSfCkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CSVLogger, Model Checkpoint(Test save model), 2-highway, Dev set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-NaXEByfCkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint('./logs/saved_models/bidaf-weights-best.h5', save_best_only = True, save_weights_only = True, mode = 'min', monitor = 'val_loss', verbose = 1)\n",
        "logger = CSVLogger('./logs/training.log', append = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0IqdlR-_fCke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(generator = dataset_train,\n",
        "                    steps_per_epoch = num_batches_per_epoch_train, \n",
        "                    epochs = 2, \n",
        "                    #validation_data = dataset_dev, \n",
        "                    #validation_steps = num_batches_per_epoch_dev,\n",
        "                    #workers = 8,\n",
        "                    #use_multiprocessing = True,\n",
        "                    shuffle = True,\n",
        "                    callbacks = [checkpoint, logger])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rybFTrt7fCkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}